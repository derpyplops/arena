{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "gpt = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt:  Jingle bells, jingle bells, jingle all the way\n",
      "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Greedy decoding a second time (should be deterministic): \n",
      "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(logits: t.Tensor) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: the most likely token (as an integer)\n",
    "    '''\n",
    "\n",
    "    return t.argmax(logits, dim=0)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(\"Greedy decoding with prompt: \", prompt)\n",
    "output = sample_tokens(gpt, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output}\")\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Greedy decoding a second time (should be deterministic): \")\n",
    "output = sample_tokens(gpt, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output}\")\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0985, 0.2028, 0.2975, 0.4012]) tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def sample_basic(logits: t.Tensor) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "\n",
    "    Return: a sampled token\n",
    "    '''\n",
    "    return Categorical(logits.softmax(0)).sample().item()\n",
    "    \n",
    "\n",
    "N = 20000\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_basic(unnormalized_logits) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts, probs)\n",
    "t.testing.assert_close(counts, probs, atol=0.01, rtol=0)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def apply_temperature(logits: t.Tensor, temperature: float) -> t.Tensor:\n",
    "    '''\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: shape (vocab_size, )\n",
    "    '''\n",
    "    assert temperature > 0\n",
    "    return logits / temperature\n",
    "\n",
    "logits = t.tensor([1, 2]).log()\n",
    "cold_logits = apply_temperature(logits, 0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "hot_logits = apply_temperature(logits, 1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "from torch import dtype\n",
    "\n",
    "def apply_freq_penalty(input_ids: t.Tensor, logits: t.Tensor, freq_penalty: float) -> t.Tensor:\n",
    "    '''\n",
    "    input_ids: shape (seq, )\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: shape (vocab_size, )\n",
    "    '''\n",
    "    # (minlen,) = logits.shape\n",
    "    # r = t.bincount(input_ids, minlength=minlen)\n",
    "    (vocab_size,) = logits.shape\n",
    "    input_ids = input_ids.squeeze_()\n",
    "    id_freqs = t.bincount(input_ids, minlength=vocab_size)\n",
    "    return logits - freq_penalty * id_freqs\n",
    "\n",
    "\n",
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = apply_freq_penalty(input_ids, logits, 2.0)\n",
    "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space\"\n",
    "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 with: High freq penalty ({'freq_penalty': 100.0}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way to 3 Mont 4.9 1 44.. tom moss Webb 2 41 80 box 42 1971-87 DC 9sc c'\n",
      "\n",
      "Sample 0 with: Negative freq penalty ({'freq_penalty': -1.0}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way across the room, you know, as the doors, you know, open, you know, and, you know,'\n",
      "\n",
      "Sample 0 with: Too hot! ({'temperature': 2.0}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way […] Memory Edgar Dannypac »player half involved ^ modulationÂ friendships Stigrated Cover by Ones 66 Charges CRC Theondur'\n",
      "\n",
      "Sample 0 with: Pleasantly cool ({'temperature': 0.7}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way! \"\\n\\n\"I... I\\'m not going to get your heads hurt,\" the female voice said.\\n\\n'\n",
      "\n",
      "Sample 0 with: Pleasantly warm ({'temperature': 0.9}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way to the top level. You\\'ve got to get bigger and better, that\\'s why we make the environment our playground.\"'\n",
      "\n",
      "Sample 0 with: Too cold! ({'temperature': 0.01}):\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\\n\\nThe first time I saw the mountain, I was in the middle of'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 1\n",
    "your_prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "cases = [\n",
    "    (\"High freq penalty\", dict(freq_penalty=100.0)),\n",
    "    (\"Negative freq penalty\", dict(freq_penalty=-1.0)),\n",
    "    (\"Too hot!\", dict(temperature=2.0)),\n",
    "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
    "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
    "    (\"Too cold!\", dict(temperature=0.01)),\n",
    "]\n",
    "for (name, kwargs) in cases:\n",
    "    for i in range(N_RUNS):\n",
    "        output = sample_tokens(gpt, tokenizer, your_prompt, max_tokens_generated=24, **kwargs)\n",
    "        print(f\"Sample {i} with: {name} ({kwargs}):\")\n",
    "        print(f\"Your model said: {repr(output)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [164], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m probs \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     19\u001b[0m unnormalized_logits \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mlog() \u001b[39m+\u001b[39m \u001b[39m1.2345\u001b[39m\n\u001b[0;32m---> 20\u001b[0m samples \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor([sample_top_k(unnormalized_logits, k) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)])\n\u001b[1;32m     21\u001b[0m counts \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mbincount(samples, minlength\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(probs)) \u001b[39m/\u001b[39m N\n\u001b[1;32m     22\u001b[0m expected \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mclone()\n",
      "Cell \u001b[0;32mIn [164], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m probs \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     19\u001b[0m unnormalized_logits \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mlog() \u001b[39m+\u001b[39m \u001b[39m1.2345\u001b[39m\n\u001b[0;32m---> 20\u001b[0m samples \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor([sample_top_k(unnormalized_logits, k) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)])\n\u001b[1;32m     21\u001b[0m counts \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mbincount(samples, minlength\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(probs)) \u001b[39m/\u001b[39m N\n\u001b[1;32m     22\u001b[0m expected \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mclone()\n",
      "Cell \u001b[0;32mIn [164], line 12\u001b[0m, in \u001b[0;36msample_top_k\u001b[0;34m(logits, top_k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_top_k\u001b[39m(logits: t\u001b[39m.\u001b[39mTensor, top_k: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    logits: shape (vocab_size, ) - unnormalized log-probabilities\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    top_k: only consider this many of the most likely tokens for sampling\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m    - Normalize and sample\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     top_logits, top_idx \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mtopk(top_k)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     13\u001b[0m     idx \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mcategorical\u001b[39m.\u001b[39mCategorical(logits\u001b[39m=\u001b[39mtop_logits)\u001b[39m.\u001b[39msample()\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m top_idx[idx]\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def sample_top_k(logits: t.Tensor, top_k: int) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "    top_k: only consider this many of the most likely tokens for sampling\n",
    "\n",
    "    Return: a sampled token\n",
    "\n",
    "    - Find the top_k largest probabilities\n",
    "    - Set all other probabilities to zero\n",
    "    - Normalize and sample\n",
    "    '''\n",
    "    top_logits, top_idx = logits.topk(top_k).values\n",
    "    idx = t.distributions.categorical.Categorical(logits=top_logits).sample()\n",
    "    return top_idx[idx].item()\n",
    "\n",
    "\n",
    "k = 3\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_top_k(unnormalized_logits, k) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "expected = probs.clone()\n",
    "expected[:-k] = 0\n",
    "expected /= expected.sum()\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
    "t.testing.assert_close(counts, expected, atol=0.01, rtol=0)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model said: 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\"The unicorns are speaking about the human race,\" said Tanya Segal, a professor of linguistics at the University of California, Santa Barbara.\\n\\nAnthropologist and researcher, Segal studied the unicorns in a lab at the University of California, San Diego and has studied the animals since'\n"
     ]
    }
   ],
   "source": [
    "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "output = sample_tokens(gpt, tokenizer, your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
    "print(f\"Your model said: {repr(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.searchsorted(t.tensor([1, 2, 3, 4, 5]), t.tensor([2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p of 0.5 or lower should only return token 2:  tensor([0., 0., 1.])\n",
      "top_p in (0.5, 0.8] should return tokens 1 and 2:  tensor([0.0000, 0.3695, 0.6305])\n",
      "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2250, 0.3338, 0.4412])\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_top_p(logits: t.Tensor, top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "\n",
    "    Return: a sampled token\n",
    "\n",
    "    Instructions:\n",
    "    Sort the probabilities from largest to smallest\n",
    "    Find the cutoff point where the cumulative probability first equals or exceeds top_p. We do the cutoff inclusively, keeping the first probability above the threshold.\n",
    "    If the number of kept probabilities is less than min_tokens_to_keep, keep that many tokens instead.\n",
    "    Set all other probabilities to zero\n",
    "    Normalize and samplex\n",
    "    '''\n",
    "\n",
    "    sorted_logits, sorted_idx = logits.sort(descending=True, stable=True)\n",
    "    cum_probs = sorted_logits.softmax(dim=0).cumsum(dim=0)\n",
    "    cutoff = t.searchsorted(cum_probs, t.tensor([top_p])) + 1\n",
    "    cutoff = max(cutoff, min_tokens_to_keep)\n",
    "    cutoff_logits = sorted_logits[:cutoff]\n",
    "    idx = t.distributions.categorical.Categorical(logits=cutoff_logits).sample()\n",
    "    return sorted_idx[idx].item()\n",
    "    \n",
    "\n",
    "N = 2000\n",
    "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, 0.5) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
    "print(\"top_p of 0.5 or lower should only return token 2: \", counts)\n",
    "assert counts[0] == 0 and counts[1] == 0\n",
    "\n",
    "N = 2000\n",
    "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, 0.50001) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
    "print(\"top_p in (0.5, 0.8] should return tokens 1 and 2: \", counts)\n",
    "assert counts[0] == 0\n",
    "\n",
    "N = 4000\n",
    "top_p = 0.71\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, top_p) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "expected = probs.clone()\n",
    "expected[0:2] = 0\n",
    "expected /= expected.sum()\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
    "t.testing.assert_close(counts, expected, atol=0.01, rtol=0.0)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def apply_sampling_methods(\n",
    "    input_ids: t.Tensor, logits: t.Tensor, temperature=1.0, freq_penalty=0.0, top_k=0, top_p=0.0\n",
    ") -> int:\n",
    "    '''\n",
    "    Return the next token, sampled from the model's probability distribution with modifiers.\n",
    "x\n",
    "    input_ids: shape (seq,)\n",
    "    '''\n",
    "    assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "    assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "    assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "    assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "    assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "    if temperature == 0:\n",
    "        return greedy_search(logits)\n",
    "    if temperature != 1.0:\n",
    "        logits = apply_temperature(logits, temperature)\n",
    "    if freq_penalty != 0.0:\n",
    "        logits = apply_freq_penalty(input_ids, logits, freq_penalty)\n",
    "    if top_k > 0:\n",
    "        return sample_top_k(logits, top_k)\n",
    "    if top_p > 0:\n",
    "        return sample_top_p(logits, top_p)\n",
    "    return sample_basic(logits)\n",
    "\n",
    "def sample_tokens(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    initial_text: str,\n",
    "    max_tokens_generated=30,\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    '''\n",
    "    Sample tokens until the model outputs `tokenizer.eos_token_id` or the specified token limit is reached.\n",
    "\n",
    "    Return: the prompt and continuation concatenated\n",
    "    '''\n",
    "    model.eval()\n",
    "    input_ids: list = tokenizer.encode(initial_text)\n",
    "    generated = []\n",
    "    device = next(model.parameters()).device\n",
    "    for _ in range(max_tokens_generated):\n",
    "        new_input_ids = t.tensor(input_ids + generated, dtype=t.int64, device=device)\n",
    "        logits = model(new_input_ids.unsqueeze(0)).logits[0, -1]\n",
    "        new_token = apply_sampling_methods(new_input_ids, logits, **kwargs)\n",
    "        generated.append(new_token)\n",
    "        if new_token == getattr(tokenizer, \"eos_token_id\", None):\n",
    "            break\n",
    "    return tokenizer.decode(input_ids + generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
