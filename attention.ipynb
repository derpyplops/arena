{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fancy_einsum import einsum\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def singlehead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: t.Tensor):\n",
    "    '''\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    mask: shape (b, s, s)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    if mask is not None:\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def multihead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "    '''\n",
    "    Q: shape (b, s1, e)\n",
    "    K: shape (b, s2, e)\n",
    "    V: shape (b, s2, e)\n",
    "\n",
    "    e = nheads * h\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    h = hidden\n",
    "\n",
    "    Return: shape (b s e)\n",
    "    '''\n",
    "\n",
    "    assert Q.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] % n_heads == 0\n",
    "    assert V.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "    # mask for autoencoder\n",
    "    mask = t.triu(t.ones(Q.shape[1], K.shape[1]), diagonal=1).bool()\n",
    "\n",
    "    Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "    scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / math.sqrt(Q.shape[-1])\n",
    "    # if mask is not None:\n",
    "    #     if mask.dim() == 2:\n",
    "    #         mask = repeat(mask, 's1 s2 -> b s1 s2', b=Q.shape[0])\n",
    "    #     else:\n",
    "    #         mask = mask.unsqueeze(1)\n",
    "    #     # print(mask.shape, scaled_dot_prod.shape)\n",
    "    #     scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 1, -1e9)\n",
    "    mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "    scaled_dot_prod += mask_filter\n",
    "    attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "    attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "    attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "    return attention\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = multihead_attention(Q, K, V, self.num_heads)\n",
    "        return att # self.W_O(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.2070, 15.0070, 15.8070],\n",
      "         [14.3999, 15.1999, 15.9999],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000]],\n",
      "\n",
      "        [[31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000]]])\n"
     ]
    }
   ],
   "source": [
    "# test single_head_attention\n",
    "Q = t.arange(2 * 7 * 3).reshape(2, 7, 3).type(t.float32)\n",
    "K = Q * 0.5\n",
    "V = Q * 0.8\n",
    "print(singlehead_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000],\n",
       "         [14.5878],\n",
       "         [13.9747],\n",
       "         [13.2046],\n",
       "         [12.4225],\n",
       "         [11.6769],\n",
       "         [10.9564],\n",
       "         [10.2500],\n",
       "         [ 9.5519],\n",
       "         [ 8.8588]],\n",
       "\n",
       "        [[ 8.1579],\n",
       "         [ 7.4807],\n",
       "         [ 6.7942],\n",
       "         [ 6.1084],\n",
       "         [ 5.4231],\n",
       "         [ 4.7382],\n",
       "         [ 4.0535],\n",
       "         [ 3.3690],\n",
       "         [ 2.6846],\n",
       "         [ 2.0003]]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = t.linspace(0, 10, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "K = t.linspace(5, 20, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "V = t.linspace(15, 2, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "# b = 2, s = 5, c = 4\n",
    "multihead_attention(Q, K, V, n_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.9540,   1.5843,   1.6350,  -1.2948,  -1.5780,  -2.8298],\n",
       "         [ -2.4249,   1.7430,   1.5237,  -1.3046,  -1.5791,  -2.8434],\n",
       "         [ -5.6502,   2.8301,   0.7615,  -1.2949,  -1.5780,  -2.8299]],\n",
       "\n",
       "        [[-13.5085,   5.4788,  -1.0956, -18.9775,  -3.4824, -27.3478],\n",
       "         [-17.2725,   6.7475,  -1.9851, -18.9775,  -3.4824, -27.3478],\n",
       "         [-21.1983,   8.0707,  -2.9128, -18.9775,  -3.4824, -27.3478]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = 'cpu'\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 6,\n",
    "    num_heads = 4,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        d = d_model\n",
    "        L = max_len\n",
    "        D = d / 2\n",
    "\n",
    "        angles = t.outer(t.arange(L), 1 / 10000 ** (2 * t.arange(D) / D))\n",
    "\n",
    "        array_2d = t.zeros((L, d))\n",
    "        array_2d[:, ::2] = t.sin(angles)\n",
    "        array_2d[:, 1::2] = t.cos(angles)\n",
    "        self.encoding = array_2d\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
    "        ''' \n",
    "        batch_size, seq_len, embedding_dim = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        att = self.attention(x) + x\n",
    "        h1 = self.layernorm1(att)\n",
    "        h2 = self.layernorm2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        decoders = [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size) # why? come back to this later\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_embedding = PositionalEncoding(config.hidden_size)\n",
    "        self.last_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedding = self.embed(tokens) # (seq_len) -> (seq_len, embedding)\n",
    "        pos_embedding = self.positional_embedding(embedding)\n",
    "        final_embedding = embedding + pos_embedding\n",
    "        a = self.dropout(final_embedding)\n",
    "        b = self.decoderlayer(a)\n",
    "        c = self.layernorm(b) @ self.embed.weight.T\n",
    "        return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    \"\"\"A toy dataset to train a model to predict\n",
    "     a random sequence of tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = 25\n",
    "        self.total_size = 1000\n",
    "        self.text = t.randint(0,config.vocab_size, (self.total_size, self.seq_len)).to(config.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.text[idx,1:]\n",
    "        text = self.text[idx,:-1]\n",
    "        return (text, label)\n",
    "\n",
    "class ReversedNumbers(Dataset):\n",
    "    def __init__(self, vocab_size: int, seq_len: int, datasize: int):\n",
    "        self.seqs = t.randint(0, vocab_size, (datasize, seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            input = self.seqs[idx]\n",
    "            target = t.flip(input, dims=(0,))\n",
    "            return (input, target)\n",
    "\n",
    "# class ShakespeareDataset(Dataset):\n",
    "#     def __init__(self, config):\n",
    "#         self.data = open('shakespeare.txt', 'r').read()\n",
    "#         self.config = config\n",
    "#         chars = sorted(set(self.data))\n",
    "#         self.vocab_size = len(chars)\n",
    "#         self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "#         self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#         print('data has %d characters, %d unique.' % (len(self.data), self.vocab_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.char_to_idx[self.data[index]]\n",
    "#         x = t.tensor([x])\n",
    "#         x = F.one_hot(x, num_classes=self.vocab_size)\n",
    "#         x = x.type(t.FloatTensor)\n",
    "#         t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n",
    "#         t = t.tensor([t])\n",
    "#         return (x.to(self.config.device), t.to(self.config.device))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def params(self):\n",
    "#         return self.vocab_size, self.char_to_idx, self.idx_to_char\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "# dummy_ds = TestDataSet(config)\n",
    "# dummy_dl = DataLoader(dummy_ds, batch_size=64, shuffle=True)\n",
    "nums_ds = ReversedNumbers(vocab_size=10, seq_len=6, datasize=10000)\n",
    "train_ds, val_ds = random_split(nums_ds, [8000, 2000])\n",
    "nums_dl = DataLoader(train_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.14454\n",
      "[1,    40] loss: 0.04006\n",
      "[1,    60] loss: 0.03013\n",
      "[1,    80] loss: 0.02848\n",
      "[1,   100] loss: 0.02743\n",
      "[1,   120] loss: 0.02687\n",
      "[2,    20] loss: 0.02618\n",
      "[2,    40] loss: 0.02559\n",
      "[2,    60] loss: 0.02511\n",
      "[2,    80] loss: 0.02478\n",
      "[2,   100] loss: 0.02456\n",
      "[2,   120] loss: 0.02429\n",
      "[3,    20] loss: 0.02415\n",
      "[3,    40] loss: 0.02382\n",
      "[3,    60] loss: 0.02363\n",
      "[3,    80] loss: 0.02350\n",
      "[3,   100] loss: 0.02349\n",
      "[3,   120] loss: 0.02334\n",
      "[4,    20] loss: 0.02325\n",
      "[4,    40] loss: 0.02320\n",
      "[4,    60] loss: 0.02317\n",
      "[4,    80] loss: 0.02300\n",
      "[4,   100] loss: 0.02284\n",
      "[4,   120] loss: 0.02274\n",
      "[5,    20] loss: 0.02277\n",
      "[5,    40] loss: 0.02265\n",
      "[5,    60] loss: 0.02273\n",
      "[5,    80] loss: 0.02262\n",
      "[5,   100] loss: 0.02252\n",
      "[5,   120] loss: 0.02248\n",
      "[6,    20] loss: 0.02254\n",
      "[6,    40] loss: 0.02235\n",
      "[6,    60] loss: 0.02233\n",
      "[6,    80] loss: 0.02229\n",
      "[6,   100] loss: 0.02225\n",
      "[6,   120] loss: 0.02209\n",
      "[7,    20] loss: 0.02221\n",
      "[7,    40] loss: 0.02201\n",
      "[7,    60] loss: 0.02202\n",
      "[7,    80] loss: 0.02213\n",
      "[7,   100] loss: 0.02204\n",
      "[7,   120] loss: 0.02178\n",
      "[8,    20] loss: 0.02178\n",
      "[8,    40] loss: 0.02169\n",
      "[8,    60] loss: 0.02170\n",
      "[8,    80] loss: 0.02159\n",
      "[8,   100] loss: 0.02155\n",
      "[8,   120] loss: 0.02144\n",
      "[9,    20] loss: 0.02134\n",
      "[9,    40] loss: 0.02140\n",
      "[9,    60] loss: 0.02106\n",
      "[9,    80] loss: 0.02104\n",
      "[9,   100] loss: 0.02091\n",
      "[9,   120] loss: 0.02086\n",
      "[10,    20] loss: 0.02079\n",
      "[10,    40] loss: 0.02096\n",
      "[10,    60] loss: 0.02085\n",
      "[10,    80] loss: 0.02061\n",
      "[10,   100] loss: 0.02041\n",
      "[10,   120] loss: 0.02044\n",
      "[11,    20] loss: 0.02023\n",
      "[11,    40] loss: 0.02030\n",
      "[11,    60] loss: 0.02045\n",
      "[11,    80] loss: 0.02010\n",
      "[11,   100] loss: 0.02007\n",
      "[11,   120] loss: 0.01989\n",
      "[12,    20] loss: 0.01974\n",
      "[12,    40] loss: 0.01987\n",
      "[12,    60] loss: 0.01980\n",
      "[12,    80] loss: 0.01967\n",
      "[12,   100] loss: 0.01955\n",
      "[12,   120] loss: 0.01949\n",
      "[13,    20] loss: 0.01933\n",
      "[13,    40] loss: 0.01928\n",
      "[13,    60] loss: 0.01932\n",
      "[13,    80] loss: 0.01905\n",
      "[13,   100] loss: 0.01899\n",
      "[13,   120] loss: 0.01908\n",
      "[14,    20] loss: 0.01871\n",
      "[14,    40] loss: 0.01878\n",
      "[14,    60] loss: 0.01870\n",
      "[14,    80] loss: 0.01844\n",
      "[14,   100] loss: 0.01835\n",
      "[14,   120] loss: 0.01844\n",
      "[15,    20] loss: 0.01818\n",
      "[15,    40] loss: 0.01812\n",
      "[15,    60] loss: 0.01803\n",
      "[15,    80] loss: 0.01799\n",
      "[15,   100] loss: 0.01789\n",
      "[15,   120] loss: 0.01781\n",
      "[16,    20] loss: 0.01767\n",
      "[16,    40] loss: 0.01755\n",
      "[16,    60] loss: 0.01749\n",
      "[16,    80] loss: 0.01739\n",
      "[16,   100] loss: 0.01738\n",
      "[16,   120] loss: 0.01737\n",
      "[17,    20] loss: 0.01723\n",
      "[17,    40] loss: 0.01700\n",
      "[17,    60] loss: 0.01700\n",
      "[17,    80] loss: 0.01694\n",
      "[17,   100] loss: 0.01670\n",
      "[17,   120] loss: 0.01661\n",
      "[18,    20] loss: 0.01642\n",
      "[18,    40] loss: 0.01641\n",
      "[18,    60] loss: 0.01618\n",
      "[18,    80] loss: 0.01606\n",
      "[18,   100] loss: 0.01601\n",
      "[18,   120] loss: 0.01594\n",
      "[19,    20] loss: 0.01560\n",
      "[19,    40] loss: 0.01539\n",
      "[19,    60] loss: 0.01544\n",
      "[19,    80] loss: 0.01526\n",
      "[19,   100] loss: 0.01511\n",
      "[19,   120] loss: 0.01500\n",
      "[20,    20] loss: 0.01486\n",
      "[20,    40] loss: 0.01467\n",
      "[20,    60] loss: 0.01475\n",
      "[20,    80] loss: 0.01471\n",
      "[20,   100] loss: 0.01458\n",
      "[20,   120] loss: 0.01467\n",
      "[21,    20] loss: 0.01441\n",
      "[21,    40] loss: 0.01433\n",
      "[21,    60] loss: 0.01443\n",
      "[21,    80] loss: 0.01436\n",
      "[21,   100] loss: 0.01415\n",
      "[21,   120] loss: 0.01417\n",
      "[22,    20] loss: 0.01405\n",
      "[22,    40] loss: 0.01414\n",
      "[22,    60] loss: 0.01401\n",
      "[22,    80] loss: 0.01379\n",
      "[22,   100] loss: 0.01391\n",
      "[22,   120] loss: 0.01395\n",
      "[23,    20] loss: 0.01376\n",
      "[23,    40] loss: 0.01393\n",
      "[23,    60] loss: 0.01372\n",
      "[23,    80] loss: 0.01379\n",
      "[23,   100] loss: 0.01364\n",
      "[23,   120] loss: 0.01369\n",
      "[24,    20] loss: 0.01359\n",
      "[24,    40] loss: 0.01347\n",
      "[24,    60] loss: 0.01350\n",
      "[24,    80] loss: 0.01361\n",
      "[24,   100] loss: 0.01360\n",
      "[24,   120] loss: 0.01353\n",
      "[25,    20] loss: 0.01360\n",
      "[25,    40] loss: 0.01340\n",
      "[25,    60] loss: 0.01340\n",
      "[25,    80] loss: 0.01326\n",
      "[25,   100] loss: 0.01331\n",
      "[25,   120] loss: 0.01325\n",
      "[26,    20] loss: 0.01335\n",
      "[26,    40] loss: 0.01341\n",
      "[26,    60] loss: 0.01321\n",
      "[26,    80] loss: 0.01322\n",
      "[26,   100] loss: 0.01311\n",
      "[26,   120] loss: 0.01326\n",
      "[27,    20] loss: 0.01319\n",
      "[27,    40] loss: 0.01315\n",
      "[27,    60] loss: 0.01309\n",
      "[27,    80] loss: 0.01318\n",
      "[27,   100] loss: 0.01303\n",
      "[27,   120] loss: 0.01313\n",
      "[28,    20] loss: 0.01302\n",
      "[28,    40] loss: 0.01319\n",
      "[28,    60] loss: 0.01307\n",
      "[28,    80] loss: 0.01300\n",
      "[28,   100] loss: 0.01303\n",
      "[28,   120] loss: 0.01303\n",
      "[29,    20] loss: 0.01297\n",
      "[29,    40] loss: 0.01288\n",
      "[29,    60] loss: 0.01277\n",
      "[29,    80] loss: 0.01295\n",
      "[29,   100] loss: 0.01284\n",
      "[29,   120] loss: 0.01294\n",
      "[30,    20] loss: 0.01289\n",
      "[30,    40] loss: 0.01286\n",
      "[30,    60] loss: 0.01290\n",
      "[30,    80] loss: 0.01290\n",
      "[30,   100] loss: 0.01280\n",
      "[30,   120] loss: 0.01280\n",
      "[31,    20] loss: 0.01264\n",
      "[31,    40] loss: 0.01278\n",
      "[31,    60] loss: 0.01281\n",
      "[31,    80] loss: 0.01264\n",
      "[31,   100] loss: 0.01268\n",
      "[31,   120] loss: 0.01265\n",
      "[32,    20] loss: 0.01270\n",
      "[32,    40] loss: 0.01269\n",
      "[32,    60] loss: 0.01270\n",
      "[32,    80] loss: 0.01273\n",
      "[32,   100] loss: 0.01259\n",
      "[32,   120] loss: 0.01247\n",
      "[33,    20] loss: 0.01268\n",
      "[33,    40] loss: 0.01265\n",
      "[33,    60] loss: 0.01275\n",
      "[33,    80] loss: 0.01282\n",
      "[33,   100] loss: 0.01265\n",
      "[33,   120] loss: 0.01250\n",
      "[34,    20] loss: 0.01270\n",
      "[34,    40] loss: 0.01265\n",
      "[34,    60] loss: 0.01260\n",
      "[34,    80] loss: 0.01247\n",
      "[34,   100] loss: 0.01238\n",
      "[34,   120] loss: 0.01258\n",
      "[35,    20] loss: 0.01245\n",
      "[35,    40] loss: 0.01246\n",
      "[35,    60] loss: 0.01249\n",
      "[35,    80] loss: 0.01256\n",
      "[35,   100] loss: 0.01249\n",
      "[35,   120] loss: 0.01251\n",
      "[36,    20] loss: 0.01249\n",
      "[36,    40] loss: 0.01250\n",
      "[36,    60] loss: 0.01256\n",
      "[36,    80] loss: 0.01243\n",
      "[36,   100] loss: 0.01240\n",
      "[36,   120] loss: 0.01241\n",
      "[37,    20] loss: 0.01243\n",
      "[37,    40] loss: 0.01238\n",
      "[37,    60] loss: 0.01238\n",
      "[37,    80] loss: 0.01243\n",
      "[37,   100] loss: 0.01245\n",
      "[37,   120] loss: 0.01235\n",
      "[38,    20] loss: 0.01238\n",
      "[38,    40] loss: 0.01233\n",
      "[38,    60] loss: 0.01242\n",
      "[38,    80] loss: 0.01247\n",
      "[38,   100] loss: 0.01233\n",
      "[38,   120] loss: 0.01229\n",
      "[39,    20] loss: 0.01229\n",
      "[39,    40] loss: 0.01239\n",
      "[39,    60] loss: 0.01224\n",
      "[39,    80] loss: 0.01227\n",
      "[39,   100] loss: 0.01230\n",
      "[39,   120] loss: 0.01226\n",
      "[40,    20] loss: 0.01231\n",
      "[40,    40] loss: 0.01224\n",
      "[40,    60] loss: 0.01235\n",
      "[40,    80] loss: 0.01224\n",
      "[40,   100] loss: 0.01222\n",
      "[40,   120] loss: 0.01226\n",
      "[41,    20] loss: 0.01230\n",
      "[41,    40] loss: 0.01226\n",
      "[41,    60] loss: 0.01221\n",
      "[41,    80] loss: 0.01226\n",
      "[41,   100] loss: 0.01222\n",
      "[41,   120] loss: 0.01219\n",
      "[42,    20] loss: 0.01217\n",
      "[42,    40] loss: 0.01221\n",
      "[42,    60] loss: 0.01221\n",
      "[42,    80] loss: 0.01222\n",
      "[42,   100] loss: 0.01219\n",
      "[42,   120] loss: 0.01229\n",
      "[43,    20] loss: 0.01217\n",
      "[43,    40] loss: 0.01211\n",
      "[43,    60] loss: 0.01225\n",
      "[43,    80] loss: 0.01220\n",
      "[43,   100] loss: 0.01214\n",
      "[43,   120] loss: 0.01215\n",
      "[44,    20] loss: 0.01214\n",
      "[44,    40] loss: 0.01210\n",
      "[44,    60] loss: 0.01206\n",
      "[44,    80] loss: 0.01214\n",
      "[44,   100] loss: 0.01219\n",
      "[44,   120] loss: 0.01215\n",
      "[45,    20] loss: 0.01208\n",
      "[45,    40] loss: 0.01220\n",
      "[45,    60] loss: 0.01213\n",
      "[45,    80] loss: 0.01212\n",
      "[45,   100] loss: 0.01217\n",
      "[45,   120] loss: 0.01219\n",
      "[46,    20] loss: 0.01209\n",
      "[46,    40] loss: 0.01211\n",
      "[46,    60] loss: 0.01206\n",
      "[46,    80] loss: 0.01210\n",
      "[46,   100] loss: 0.01201\n",
      "[46,   120] loss: 0.01224\n",
      "[47,    20] loss: 0.01214\n",
      "[47,    40] loss: 0.01203\n",
      "[47,    60] loss: 0.01207\n",
      "[47,    80] loss: 0.01211\n",
      "[47,   100] loss: 0.01202\n",
      "[47,   120] loss: 0.01201\n",
      "[48,    20] loss: 0.01204\n",
      "[48,    40] loss: 0.01216\n",
      "[48,    60] loss: 0.01200\n",
      "[48,    80] loss: 0.01195\n",
      "[48,   100] loss: 0.01209\n",
      "[48,   120] loss: 0.01207\n",
      "[49,    20] loss: 0.01210\n",
      "[49,    40] loss: 0.01205\n",
      "[49,    60] loss: 0.01214\n",
      "[49,    80] loss: 0.01204\n",
      "[49,   100] loss: 0.01201\n",
      "[49,   120] loss: 0.01200\n",
      "[50,    20] loss: 0.01202\n",
      "[50,    40] loss: 0.01187\n",
      "[50,    60] loss: 0.01197\n",
      "[50,    80] loss: 0.01206\n",
      "[50,   100] loss: 0.01200\n",
      "[50,   120] loss: 0.01200\n",
      "[51,    20] loss: 0.01194\n",
      "[51,    40] loss: 0.01200\n",
      "[51,    60] loss: 0.01192\n",
      "[51,    80] loss: 0.01200\n",
      "[51,   100] loss: 0.01208\n",
      "[51,   120] loss: 0.01191\n",
      "[52,    20] loss: 0.01200\n",
      "[52,    40] loss: 0.01195\n",
      "[52,    60] loss: 0.01196\n",
      "[52,    80] loss: 0.01198\n",
      "[52,   100] loss: 0.01200\n",
      "[52,   120] loss: 0.01196\n",
      "[53,    20] loss: 0.01190\n",
      "[53,    40] loss: 0.01191\n",
      "[53,    60] loss: 0.01192\n",
      "[53,    80] loss: 0.01195\n",
      "[53,   100] loss: 0.01201\n",
      "[53,   120] loss: 0.01184\n",
      "[54,    20] loss: 0.01188\n",
      "[54,    40] loss: 0.01192\n",
      "[54,    60] loss: 0.01198\n",
      "[54,    80] loss: 0.01193\n",
      "[54,   100] loss: 0.01184\n",
      "[54,   120] loss: 0.01193\n",
      "[55,    20] loss: 0.01193\n",
      "[55,    40] loss: 0.01182\n",
      "[55,    60] loss: 0.01184\n",
      "[55,    80] loss: 0.01194\n",
      "[55,   100] loss: 0.01188\n",
      "[55,   120] loss: 0.01200\n",
      "[56,    20] loss: 0.01192\n",
      "[56,    40] loss: 0.01189\n",
      "[56,    60] loss: 0.01188\n",
      "[56,    80] loss: 0.01191\n",
      "[56,   100] loss: 0.01198\n",
      "[56,   120] loss: 0.01196\n",
      "[57,    20] loss: 0.01186\n",
      "[57,    40] loss: 0.01188\n",
      "[57,    60] loss: 0.01182\n",
      "[57,    80] loss: 0.01196\n",
      "[57,   100] loss: 0.01189\n",
      "[57,   120] loss: 0.01192\n",
      "[58,    20] loss: 0.01188\n",
      "[58,    40] loss: 0.01187\n",
      "[58,    60] loss: 0.01191\n",
      "[58,    80] loss: 0.01199\n",
      "[58,   100] loss: 0.01186\n",
      "[58,   120] loss: 0.01188\n",
      "[59,    20] loss: 0.01186\n",
      "[59,    40] loss: 0.01187\n",
      "[59,    60] loss: 0.01186\n",
      "[59,    80] loss: 0.01190\n",
      "[59,   100] loss: 0.01186\n",
      "[59,   120] loss: 0.01191\n",
      "[60,    20] loss: 0.01178\n",
      "[60,    40] loss: 0.01188\n",
      "[60,    60] loss: 0.01180\n",
      "[60,    80] loss: 0.01184\n",
      "[60,   100] loss: 0.01182\n",
      "[60,   120] loss: 0.01184\n",
      "[61,    20] loss: 0.01186\n",
      "[61,    40] loss: 0.01187\n",
      "[61,    60] loss: 0.01190\n",
      "[61,    80] loss: 0.01183\n",
      "[61,   100] loss: 0.01183\n",
      "[61,   120] loss: 0.01183\n",
      "[62,    20] loss: 0.01182\n",
      "[62,    40] loss: 0.01184\n",
      "[62,    60] loss: 0.01182\n",
      "[62,    80] loss: 0.01186\n",
      "[62,   100] loss: 0.01179\n",
      "[62,   120] loss: 0.01183\n",
      "[63,    20] loss: 0.01178\n",
      "[63,    40] loss: 0.01180\n",
      "[63,    60] loss: 0.01183\n",
      "[63,    80] loss: 0.01184\n",
      "[63,   100] loss: 0.01187\n",
      "[63,   120] loss: 0.01183\n",
      "[64,    20] loss: 0.01174\n",
      "[64,    40] loss: 0.01194\n",
      "[64,    60] loss: 0.01182\n",
      "[64,    80] loss: 0.01179\n",
      "[64,   100] loss: 0.01178\n",
      "[64,   120] loss: 0.01178\n",
      "[65,    20] loss: 0.01178\n",
      "[65,    40] loss: 0.01181\n",
      "[65,    60] loss: 0.01185\n",
      "[65,    80] loss: 0.01169\n",
      "[65,   100] loss: 0.01187\n",
      "[65,   120] loss: 0.01175\n",
      "[66,    20] loss: 0.01171\n",
      "[66,    40] loss: 0.01173\n",
      "[66,    60] loss: 0.01178\n",
      "[66,    80] loss: 0.01184\n",
      "[66,   100] loss: 0.01183\n",
      "[66,   120] loss: 0.01177\n",
      "[67,    20] loss: 0.01176\n",
      "[67,    40] loss: 0.01179\n",
      "[67,    60] loss: 0.01183\n",
      "[67,    80] loss: 0.01179\n",
      "[67,   100] loss: 0.01186\n",
      "[67,   120] loss: 0.01173\n",
      "[68,    20] loss: 0.01178\n",
      "[68,    40] loss: 0.01175\n",
      "[68,    60] loss: 0.01176\n",
      "[68,    80] loss: 0.01175\n",
      "[68,   100] loss: 0.01172\n",
      "[68,   120] loss: 0.01182\n",
      "[69,    20] loss: 0.01184\n",
      "[69,    40] loss: 0.01178\n",
      "[69,    60] loss: 0.01173\n",
      "[69,    80] loss: 0.01179\n",
      "[69,   100] loss: 0.01179\n",
      "[69,   120] loss: 0.01185\n",
      "[70,    20] loss: 0.01175\n",
      "[70,    40] loss: 0.01175\n",
      "[70,    60] loss: 0.01174\n",
      "[70,    80] loss: 0.01178\n",
      "[70,   100] loss: 0.01174\n",
      "[70,   120] loss: 0.01176\n",
      "[71,    20] loss: 0.01167\n",
      "[71,    40] loss: 0.01171\n",
      "[71,    60] loss: 0.01177\n",
      "[71,    80] loss: 0.01173\n",
      "[71,   100] loss: 0.01177\n",
      "[71,   120] loss: 0.01176\n",
      "[72,    20] loss: 0.01171\n",
      "[72,    40] loss: 0.01169\n",
      "[72,    60] loss: 0.01171\n",
      "[72,    80] loss: 0.01175\n",
      "[72,   100] loss: 0.01175\n",
      "[72,   120] loss: 0.01169\n",
      "[73,    20] loss: 0.01168\n",
      "[73,    40] loss: 0.01163\n",
      "[73,    60] loss: 0.01178\n",
      "[73,    80] loss: 0.01175\n",
      "[73,   100] loss: 0.01175\n",
      "[73,   120] loss: 0.01171\n",
      "[74,    20] loss: 0.01172\n",
      "[74,    40] loss: 0.01168\n",
      "[74,    60] loss: 0.01172\n",
      "[74,    80] loss: 0.01178\n",
      "[74,   100] loss: 0.01176\n",
      "[74,   120] loss: 0.01174\n",
      "[75,    20] loss: 0.01173\n",
      "[75,    40] loss: 0.01170\n",
      "[75,    60] loss: 0.01172\n",
      "[75,    80] loss: 0.01170\n",
      "[75,   100] loss: 0.01178\n",
      "[75,   120] loss: 0.01164\n",
      "[76,    20] loss: 0.01169\n",
      "[76,    40] loss: 0.01171\n",
      "[76,    60] loss: 0.01173\n",
      "[76,    80] loss: 0.01168\n",
      "[76,   100] loss: 0.01172\n",
      "[76,   120] loss: 0.01181\n",
      "[77,    20] loss: 0.01176\n",
      "[77,    40] loss: 0.01170\n",
      "[77,    60] loss: 0.01165\n",
      "[77,    80] loss: 0.01175\n",
      "[77,   100] loss: 0.01170\n",
      "[77,   120] loss: 0.01167\n",
      "[78,    20] loss: 0.01164\n",
      "[78,    40] loss: 0.01171\n",
      "[78,    60] loss: 0.01169\n",
      "[78,    80] loss: 0.01172\n",
      "[78,   100] loss: 0.01174\n",
      "[78,   120] loss: 0.01172\n",
      "[79,    20] loss: 0.01168\n",
      "[79,    40] loss: 0.01172\n",
      "[79,    60] loss: 0.01172\n",
      "[79,    80] loss: 0.01176\n",
      "[79,   100] loss: 0.01174\n",
      "[79,   120] loss: 0.01172\n",
      "[80,    20] loss: 0.01164\n",
      "[80,    40] loss: 0.01168\n",
      "[80,    60] loss: 0.01175\n",
      "[80,    80] loss: 0.01165\n",
      "[80,   100] loss: 0.01174\n",
      "[80,   120] loss: 0.01167\n",
      "[81,    20] loss: 0.01168\n",
      "[81,    40] loss: 0.01176\n",
      "[81,    60] loss: 0.01174\n",
      "[81,    80] loss: 0.01174\n",
      "[81,   100] loss: 0.01167\n",
      "[81,   120] loss: 0.01167\n",
      "[82,    20] loss: 0.01161\n",
      "[82,    40] loss: 0.01168\n",
      "[82,    60] loss: 0.01169\n",
      "[82,    80] loss: 0.01161\n",
      "[82,   100] loss: 0.01167\n",
      "[82,   120] loss: 0.01171\n",
      "[83,    20] loss: 0.01161\n",
      "[83,    40] loss: 0.01170\n",
      "[83,    60] loss: 0.01164\n",
      "[83,    80] loss: 0.01166\n",
      "[83,   100] loss: 0.01188\n",
      "[83,   120] loss: 0.01172\n",
      "[84,    20] loss: 0.01163\n",
      "[84,    40] loss: 0.01166\n",
      "[84,    60] loss: 0.01172\n",
      "[84,    80] loss: 0.01168\n",
      "[84,   100] loss: 0.01166\n",
      "[84,   120] loss: 0.01166\n",
      "[85,    20] loss: 0.01164\n",
      "[85,    40] loss: 0.01165\n",
      "[85,    60] loss: 0.01162\n",
      "[85,    80] loss: 0.01171\n",
      "[85,   100] loss: 0.01168\n",
      "[85,   120] loss: 0.01166\n",
      "[86,    20] loss: 0.01167\n",
      "[86,    40] loss: 0.01168\n",
      "[86,    60] loss: 0.01171\n",
      "[86,    80] loss: 0.01168\n",
      "[86,   100] loss: 0.01168\n",
      "[86,   120] loss: 0.01172\n",
      "[87,    20] loss: 0.01167\n",
      "[87,    40] loss: 0.01173\n",
      "[87,    60] loss: 0.01170\n",
      "[87,    80] loss: 0.01172\n",
      "[87,   100] loss: 0.01168\n",
      "[87,   120] loss: 0.01169\n",
      "[88,    20] loss: 0.01169\n",
      "[88,    40] loss: 0.01162\n",
      "[88,    60] loss: 0.01165\n",
      "[88,    80] loss: 0.01165\n",
      "[88,   100] loss: 0.01159\n",
      "[88,   120] loss: 0.01170\n",
      "[89,    20] loss: 0.01166\n",
      "[89,    40] loss: 0.01169\n",
      "[89,    60] loss: 0.01165\n",
      "[89,    80] loss: 0.01166\n",
      "[89,   100] loss: 0.01167\n",
      "[89,   120] loss: 0.01163\n",
      "[90,    20] loss: 0.01164\n",
      "[90,    40] loss: 0.01172\n",
      "[90,    60] loss: 0.01162\n",
      "[90,    80] loss: 0.01161\n",
      "[90,   100] loss: 0.01167\n",
      "[90,   120] loss: 0.01166\n",
      "[91,    20] loss: 0.01159\n",
      "[91,    40] loss: 0.01163\n",
      "[91,    60] loss: 0.01165\n",
      "[91,    80] loss: 0.01161\n",
      "[91,   100] loss: 0.01165\n",
      "[91,   120] loss: 0.01164\n",
      "[92,    20] loss: 0.01164\n",
      "[92,    40] loss: 0.01163\n",
      "[92,    60] loss: 0.01162\n",
      "[92,    80] loss: 0.01166\n",
      "[92,   100] loss: 0.01165\n",
      "[92,   120] loss: 0.01166\n",
      "[93,    20] loss: 0.01163\n",
      "[93,    40] loss: 0.01162\n",
      "[93,    60] loss: 0.01157\n",
      "[93,    80] loss: 0.01162\n",
      "[93,   100] loss: 0.01158\n",
      "[93,   120] loss: 0.01164\n",
      "[94,    20] loss: 0.01164\n",
      "[94,    40] loss: 0.01167\n",
      "[94,    60] loss: 0.01165\n",
      "[94,    80] loss: 0.01162\n",
      "[94,   100] loss: 0.01163\n",
      "[94,   120] loss: 0.01157\n",
      "[95,    20] loss: 0.01158\n",
      "[95,    40] loss: 0.01160\n",
      "[95,    60] loss: 0.01167\n",
      "[95,    80] loss: 0.01163\n",
      "[95,   100] loss: 0.01163\n",
      "[95,   120] loss: 0.01163\n",
      "[96,    20] loss: 0.01162\n",
      "[96,    40] loss: 0.01167\n",
      "[96,    60] loss: 0.01163\n",
      "[96,    80] loss: 0.01163\n",
      "[96,   100] loss: 0.01163\n",
      "[96,   120] loss: 0.01161\n",
      "[97,    20] loss: 0.01159\n",
      "[97,    40] loss: 0.01157\n",
      "[97,    60] loss: 0.01165\n",
      "[97,    80] loss: 0.01159\n",
      "[97,   100] loss: 0.01159\n",
      "[97,   120] loss: 0.01161\n",
      "[98,    20] loss: 0.01166\n",
      "[98,    40] loss: 0.01164\n",
      "[98,    60] loss: 0.01163\n",
      "[98,    80] loss: 0.01165\n",
      "[98,   100] loss: 0.01165\n",
      "[98,   120] loss: 0.01161\n",
      "[99,    20] loss: 0.01162\n",
      "[99,    40] loss: 0.01159\n",
      "[99,    60] loss: 0.01165\n",
      "[99,    80] loss: 0.01160\n",
      "[99,   100] loss: 0.01164\n",
      "[99,   120] loss: 0.01162\n",
      "[100,    20] loss: 0.01161\n",
      "[100,    40] loss: 0.01153\n",
      "[100,    60] loss: 0.01164\n",
      "[100,    80] loss: 0.01170\n",
      "[100,   100] loss: 0.01167\n",
      "[100,   120] loss: 0.01166\n",
      "[101,    20] loss: 0.01168\n",
      "[101,    40] loss: 0.01161\n",
      "[101,    60] loss: 0.01162\n",
      "[101,    80] loss: 0.01156\n",
      "[101,   100] loss: 0.01160\n",
      "[101,   120] loss: 0.01164\n",
      "[102,    20] loss: 0.01163\n",
      "[102,    40] loss: 0.01161\n",
      "[102,    60] loss: 0.01158\n",
      "[102,    80] loss: 0.01156\n",
      "[102,   100] loss: 0.01158\n",
      "[102,   120] loss: 0.01162\n",
      "[103,    20] loss: 0.01161\n",
      "[103,    40] loss: 0.01159\n",
      "[103,    60] loss: 0.01163\n",
      "[103,    80] loss: 0.01164\n",
      "[103,   100] loss: 0.01160\n",
      "[103,   120] loss: 0.01162\n",
      "[104,    20] loss: 0.01166\n",
      "[104,    40] loss: 0.01156\n",
      "[104,    60] loss: 0.01156\n",
      "[104,    80] loss: 0.01164\n",
      "[104,   100] loss: 0.01162\n",
      "[104,   120] loss: 0.01161\n",
      "[105,    20] loss: 0.01157\n",
      "[105,    40] loss: 0.01153\n",
      "[105,    60] loss: 0.01164\n",
      "[105,    80] loss: 0.01159\n",
      "[105,   100] loss: 0.01165\n",
      "[105,   120] loss: 0.01164\n",
      "[106,    20] loss: 0.01158\n",
      "[106,    40] loss: 0.01162\n",
      "[106,    60] loss: 0.01162\n",
      "[106,    80] loss: 0.01165\n",
      "[106,   100] loss: 0.01160\n",
      "[106,   120] loss: 0.01166\n",
      "[107,    20] loss: 0.01163\n",
      "[107,    40] loss: 0.01163\n",
      "[107,    60] loss: 0.01164\n",
      "[107,    80] loss: 0.01162\n",
      "[107,   100] loss: 0.01159\n",
      "[107,   120] loss: 0.01164\n",
      "[108,    20] loss: 0.01154\n",
      "[108,    40] loss: 0.01158\n",
      "[108,    60] loss: 0.01162\n",
      "[108,    80] loss: 0.01156\n",
      "[108,   100] loss: 0.01158\n",
      "[108,   120] loss: 0.01161\n",
      "[109,    20] loss: 0.01160\n",
      "[109,    40] loss: 0.01161\n",
      "[109,    60] loss: 0.01158\n",
      "[109,    80] loss: 0.01157\n",
      "[109,   100] loss: 0.01159\n",
      "[109,   120] loss: 0.01157\n",
      "[110,    20] loss: 0.01156\n",
      "[110,    40] loss: 0.01156\n",
      "[110,    60] loss: 0.01161\n",
      "[110,    80] loss: 0.01160\n",
      "[110,   100] loss: 0.01164\n",
      "[110,   120] loss: 0.01166\n",
      "[111,    20] loss: 0.01160\n",
      "[111,    40] loss: 0.01160\n",
      "[111,    60] loss: 0.01160\n",
      "[111,    80] loss: 0.01156\n",
      "[111,   100] loss: 0.01160\n",
      "[111,   120] loss: 0.01159\n",
      "[112,    20] loss: 0.01157\n",
      "[112,    40] loss: 0.01160\n",
      "[112,    60] loss: 0.01166\n",
      "[112,    80] loss: 0.01169\n",
      "[112,   100] loss: 0.01163\n",
      "[112,   120] loss: 0.01166\n",
      "[113,    20] loss: 0.01163\n",
      "[113,    40] loss: 0.01162\n",
      "[113,    60] loss: 0.01158\n",
      "[113,    80] loss: 0.01161\n",
      "[113,   100] loss: 0.01163\n",
      "[113,   120] loss: 0.01160\n",
      "[114,    20] loss: 0.01157\n",
      "[114,    40] loss: 0.01153\n",
      "[114,    60] loss: 0.01160\n",
      "[114,    80] loss: 0.01154\n",
      "[114,   100] loss: 0.01163\n",
      "[114,   120] loss: 0.01156\n",
      "[115,    20] loss: 0.01152\n",
      "[115,    40] loss: 0.01159\n",
      "[115,    60] loss: 0.01154\n",
      "[115,    80] loss: 0.01159\n",
      "[115,   100] loss: 0.01156\n",
      "[115,   120] loss: 0.01157\n",
      "[116,    20] loss: 0.01154\n",
      "[116,    40] loss: 0.01162\n",
      "[116,    60] loss: 0.01155\n",
      "[116,    80] loss: 0.01162\n",
      "[116,   100] loss: 0.01157\n",
      "[116,   120] loss: 0.01155\n",
      "[117,    20] loss: 0.01159\n",
      "[117,    40] loss: 0.01159\n",
      "[117,    60] loss: 0.01157\n",
      "[117,    80] loss: 0.01154\n",
      "[117,   100] loss: 0.01168\n",
      "[117,   120] loss: 0.01155\n",
      "[118,    20] loss: 0.01154\n",
      "[118,    40] loss: 0.01155\n",
      "[118,    60] loss: 0.01153\n",
      "[118,    80] loss: 0.01158\n",
      "[118,   100] loss: 0.01154\n",
      "[118,   120] loss: 0.01158\n",
      "[119,    20] loss: 0.01162\n",
      "[119,    40] loss: 0.01156\n",
      "[119,    60] loss: 0.01158\n",
      "[119,    80] loss: 0.01162\n",
      "[119,   100] loss: 0.01158\n",
      "[119,   120] loss: 0.01162\n",
      "[120,    20] loss: 0.01155\n",
      "[120,    40] loss: 0.01156\n",
      "[120,    60] loss: 0.01158\n",
      "[120,    80] loss: 0.01161\n",
      "[120,   100] loss: 0.01160\n",
      "[120,   120] loss: 0.01161\n",
      "[121,    20] loss: 0.01147\n",
      "[121,    40] loss: 0.01161\n",
      "[121,    60] loss: 0.01157\n",
      "[121,    80] loss: 0.01159\n",
      "[121,   100] loss: 0.01159\n",
      "[121,   120] loss: 0.01156\n",
      "[122,    20] loss: 0.01153\n",
      "[122,    40] loss: 0.01160\n",
      "[122,    60] loss: 0.01158\n",
      "[122,    80] loss: 0.01155\n",
      "[122,   100] loss: 0.01155\n",
      "[122,   120] loss: 0.01155\n",
      "[123,    20] loss: 0.01154\n",
      "[123,    40] loss: 0.01159\n",
      "[123,    60] loss: 0.01152\n",
      "[123,    80] loss: 0.01157\n",
      "[123,   100] loss: 0.01159\n",
      "[123,   120] loss: 0.01155\n",
      "[124,    20] loss: 0.01154\n",
      "[124,    40] loss: 0.01151\n",
      "[124,    60] loss: 0.01155\n",
      "[124,    80] loss: 0.01156\n",
      "[124,   100] loss: 0.01152\n",
      "[124,   120] loss: 0.01154\n",
      "[125,    20] loss: 0.01150\n",
      "[125,    40] loss: 0.01150\n",
      "[125,    60] loss: 0.01151\n",
      "[125,    80] loss: 0.01156\n",
      "[125,   100] loss: 0.01158\n",
      "[125,   120] loss: 0.01158\n",
      "[126,    20] loss: 0.01151\n",
      "[126,    40] loss: 0.01157\n",
      "[126,    60] loss: 0.01155\n",
      "[126,    80] loss: 0.01158\n",
      "[126,   100] loss: 0.01152\n",
      "[126,   120] loss: 0.01156\n",
      "[127,    20] loss: 0.01149\n",
      "[127,    40] loss: 0.01154\n",
      "[127,    60] loss: 0.01155\n",
      "[127,    80] loss: 0.01159\n",
      "[127,   100] loss: 0.01160\n",
      "[127,   120] loss: 0.01160\n",
      "[128,    20] loss: 0.01152\n",
      "[128,    40] loss: 0.01155\n",
      "[128,    60] loss: 0.01156\n",
      "[128,    80] loss: 0.01152\n",
      "[128,   100] loss: 0.01159\n",
      "[128,   120] loss: 0.01155\n",
      "[129,    20] loss: 0.01155\n",
      "[129,    40] loss: 0.01156\n",
      "[129,    60] loss: 0.01155\n",
      "[129,    80] loss: 0.01157\n",
      "[129,   100] loss: 0.01154\n",
      "[129,   120] loss: 0.01152\n",
      "[130,    20] loss: 0.01146\n",
      "[130,    40] loss: 0.01150\n",
      "[130,    60] loss: 0.01150\n",
      "[130,    80] loss: 0.01155\n",
      "[130,   100] loss: 0.01161\n",
      "[130,   120] loss: 0.01152\n",
      "[131,    20] loss: 0.01154\n",
      "[131,    40] loss: 0.01154\n",
      "[131,    60] loss: 0.01148\n",
      "[131,    80] loss: 0.01158\n",
      "[131,   100] loss: 0.01151\n",
      "[131,   120] loss: 0.01155\n",
      "[132,    20] loss: 0.01156\n",
      "[132,    40] loss: 0.01151\n",
      "[132,    60] loss: 0.01149\n",
      "[132,    80] loss: 0.01154\n",
      "[132,   100] loss: 0.01156\n",
      "[132,   120] loss: 0.01152\n",
      "[133,    20] loss: 0.01150\n",
      "[133,    40] loss: 0.01149\n",
      "[133,    60] loss: 0.01154\n",
      "[133,    80] loss: 0.01154\n",
      "[133,   100] loss: 0.01154\n",
      "[133,   120] loss: 0.01158\n",
      "[134,    20] loss: 0.01155\n",
      "[134,    40] loss: 0.01151\n",
      "[134,    60] loss: 0.01152\n",
      "[134,    80] loss: 0.01152\n",
      "[134,   100] loss: 0.01160\n",
      "[134,   120] loss: 0.01154\n",
      "[135,    20] loss: 0.01154\n",
      "[135,    40] loss: 0.01155\n",
      "[135,    60] loss: 0.01154\n",
      "[135,    80] loss: 0.01150\n",
      "[135,   100] loss: 0.01160\n",
      "[135,   120] loss: 0.01155\n",
      "[136,    20] loss: 0.01152\n",
      "[136,    40] loss: 0.01155\n",
      "[136,    60] loss: 0.01150\n",
      "[136,    80] loss: 0.01153\n",
      "[136,   100] loss: 0.01151\n",
      "[136,   120] loss: 0.01153\n",
      "[137,    20] loss: 0.01150\n",
      "[137,    40] loss: 0.01153\n",
      "[137,    60] loss: 0.01154\n",
      "[137,    80] loss: 0.01152\n",
      "[137,   100] loss: 0.01153\n",
      "[137,   120] loss: 0.01151\n",
      "[138,    20] loss: 0.01156\n",
      "[138,    40] loss: 0.01160\n",
      "[138,    60] loss: 0.01156\n",
      "[138,    80] loss: 0.01155\n",
      "[138,   100] loss: 0.01158\n",
      "[138,   120] loss: 0.01152\n",
      "[139,    20] loss: 0.01150\n",
      "[139,    40] loss: 0.01152\n",
      "[139,    60] loss: 0.01149\n",
      "[139,    80] loss: 0.01151\n",
      "[139,   100] loss: 0.01159\n",
      "[139,   120] loss: 0.01157\n",
      "[140,    20] loss: 0.01151\n",
      "[140,    40] loss: 0.01152\n",
      "[140,    60] loss: 0.01148\n",
      "[140,    80] loss: 0.01155\n",
      "[140,   100] loss: 0.01155\n",
      "[140,   120] loss: 0.01157\n",
      "[141,    20] loss: 0.01150\n",
      "[141,    40] loss: 0.01150\n",
      "[141,    60] loss: 0.01153\n",
      "[141,    80] loss: 0.01147\n",
      "[141,   100] loss: 0.01148\n",
      "[141,   120] loss: 0.01156\n",
      "[142,    20] loss: 0.01150\n",
      "[142,    40] loss: 0.01145\n",
      "[142,    60] loss: 0.01153\n",
      "[142,    80] loss: 0.01151\n",
      "[142,   100] loss: 0.01151\n",
      "[142,   120] loss: 0.01150\n",
      "[143,    20] loss: 0.01151\n",
      "[143,    40] loss: 0.01150\n",
      "[143,    60] loss: 0.01153\n",
      "[143,    80] loss: 0.01152\n",
      "[143,   100] loss: 0.01158\n",
      "[143,   120] loss: 0.01149\n",
      "[144,    20] loss: 0.01148\n",
      "[144,    40] loss: 0.01153\n",
      "[144,    60] loss: 0.01153\n",
      "[144,    80] loss: 0.01158\n",
      "[144,   100] loss: 0.01154\n",
      "[144,   120] loss: 0.01154\n",
      "[145,    20] loss: 0.01152\n",
      "[145,    40] loss: 0.01150\n",
      "[145,    60] loss: 0.01148\n",
      "[145,    80] loss: 0.01148\n",
      "[145,   100] loss: 0.01156\n",
      "[145,   120] loss: 0.01151\n",
      "[146,    20] loss: 0.01147\n",
      "[146,    40] loss: 0.01151\n",
      "[146,    60] loss: 0.01151\n",
      "[146,    80] loss: 0.01149\n",
      "[146,   100] loss: 0.01154\n",
      "[146,   120] loss: 0.01151\n",
      "[147,    20] loss: 0.01152\n",
      "[147,    40] loss: 0.01153\n",
      "[147,    60] loss: 0.01155\n",
      "[147,    80] loss: 0.01153\n",
      "[147,   100] loss: 0.01148\n",
      "[147,   120] loss: 0.01149\n",
      "[148,    20] loss: 0.01155\n",
      "[148,    40] loss: 0.01152\n",
      "[148,    60] loss: 0.01154\n",
      "[148,    80] loss: 0.01148\n",
      "[148,   100] loss: 0.01151\n",
      "[148,   120] loss: 0.01149\n",
      "[149,    20] loss: 0.01147\n",
      "[149,    40] loss: 0.01149\n",
      "[149,    60] loss: 0.01153\n",
      "[149,    80] loss: 0.01149\n",
      "[149,   100] loss: 0.01154\n",
      "[149,   120] loss: 0.01147\n",
      "[150,    20] loss: 0.01148\n",
      "[150,    40] loss: 0.01148\n",
      "[150,    60] loss: 0.01150\n",
      "[150,    80] loss: 0.01154\n",
      "[150,   100] loss: 0.01150\n",
      "[150,   120] loss: 0.01149\n",
      "[151,    20] loss: 0.01145\n",
      "[151,    40] loss: 0.01146\n",
      "[151,    60] loss: 0.01151\n",
      "[151,    80] loss: 0.01146\n",
      "[151,   100] loss: 0.01153\n",
      "[151,   120] loss: 0.01149\n",
      "[152,    20] loss: 0.01143\n",
      "[152,    40] loss: 0.01146\n",
      "[152,    60] loss: 0.01150\n",
      "[152,    80] loss: 0.01158\n",
      "[152,   100] loss: 0.01146\n",
      "[152,   120] loss: 0.01150\n",
      "[153,    20] loss: 0.01148\n",
      "[153,    40] loss: 0.01150\n",
      "[153,    60] loss: 0.01151\n",
      "[153,    80] loss: 0.01151\n",
      "[153,   100] loss: 0.01147\n",
      "[153,   120] loss: 0.01150\n",
      "[154,    20] loss: 0.01147\n",
      "[154,    40] loss: 0.01147\n",
      "[154,    60] loss: 0.01149\n",
      "[154,    80] loss: 0.01147\n",
      "[154,   100] loss: 0.01149\n",
      "[154,   120] loss: 0.01149\n",
      "[155,    20] loss: 0.01148\n",
      "[155,    40] loss: 0.01148\n",
      "[155,    60] loss: 0.01149\n",
      "[155,    80] loss: 0.01154\n",
      "[155,   100] loss: 0.01147\n",
      "[155,   120] loss: 0.01152\n",
      "[156,    20] loss: 0.01139\n",
      "[156,    40] loss: 0.01149\n",
      "[156,    60] loss: 0.01147\n",
      "[156,    80] loss: 0.01151\n",
      "[156,   100] loss: 0.01148\n",
      "[156,   120] loss: 0.01147\n",
      "[157,    20] loss: 0.01145\n",
      "[157,    40] loss: 0.01146\n",
      "[157,    60] loss: 0.01149\n",
      "[157,    80] loss: 0.01151\n",
      "[157,   100] loss: 0.01152\n",
      "[157,   120] loss: 0.01148\n",
      "[158,    20] loss: 0.01139\n",
      "[158,    40] loss: 0.01145\n",
      "[158,    60] loss: 0.01149\n",
      "[158,    80] loss: 0.01144\n",
      "[158,   100] loss: 0.01149\n",
      "[158,   120] loss: 0.01150\n",
      "[159,    20] loss: 0.01150\n",
      "[159,    40] loss: 0.01152\n",
      "[159,    60] loss: 0.01148\n",
      "[159,    80] loss: 0.01145\n",
      "[159,   100] loss: 0.01152\n",
      "[159,   120] loss: 0.01146\n",
      "[160,    20] loss: 0.01147\n",
      "[160,    40] loss: 0.01146\n",
      "[160,    60] loss: 0.01146\n",
      "[160,    80] loss: 0.01145\n",
      "[160,   100] loss: 0.01151\n",
      "[160,   120] loss: 0.01144\n",
      "[161,    20] loss: 0.01149\n",
      "[161,    40] loss: 0.01147\n",
      "[161,    60] loss: 0.01140\n",
      "[161,    80] loss: 0.01142\n",
      "[161,   100] loss: 0.01149\n",
      "[161,   120] loss: 0.01150\n",
      "[162,    20] loss: 0.01141\n",
      "[162,    40] loss: 0.01143\n",
      "[162,    60] loss: 0.01146\n",
      "[162,    80] loss: 0.01150\n",
      "[162,   100] loss: 0.01149\n",
      "[162,   120] loss: 0.01147\n",
      "[163,    20] loss: 0.01142\n",
      "[163,    40] loss: 0.01146\n",
      "[163,    60] loss: 0.01147\n",
      "[163,    80] loss: 0.01149\n",
      "[163,   100] loss: 0.01145\n",
      "[163,   120] loss: 0.01148\n",
      "[164,    20] loss: 0.01140\n",
      "[164,    40] loss: 0.01147\n",
      "[164,    60] loss: 0.01147\n",
      "[164,    80] loss: 0.01145\n",
      "[164,   100] loss: 0.01152\n",
      "[164,   120] loss: 0.01149\n",
      "[165,    20] loss: 0.01143\n",
      "[165,    40] loss: 0.01147\n",
      "[165,    60] loss: 0.01147\n",
      "[165,    80] loss: 0.01147\n",
      "[165,   100] loss: 0.01147\n",
      "[165,   120] loss: 0.01144\n",
      "[166,    20] loss: 0.01144\n",
      "[166,    40] loss: 0.01144\n",
      "[166,    60] loss: 0.01140\n",
      "[166,    80] loss: 0.01146\n",
      "[166,   100] loss: 0.01144\n",
      "[166,   120] loss: 0.01147\n",
      "[167,    20] loss: 0.01141\n",
      "[167,    40] loss: 0.01146\n",
      "[167,    60] loss: 0.01144\n",
      "[167,    80] loss: 0.01149\n",
      "[167,   100] loss: 0.01148\n",
      "[167,   120] loss: 0.01147\n",
      "[168,    20] loss: 0.01143\n",
      "[168,    40] loss: 0.01145\n",
      "[168,    60] loss: 0.01150\n",
      "[168,    80] loss: 0.01144\n",
      "[168,   100] loss: 0.01146\n",
      "[168,   120] loss: 0.01145\n",
      "[169,    20] loss: 0.01144\n",
      "[169,    40] loss: 0.01144\n",
      "[169,    60] loss: 0.01143\n",
      "[169,    80] loss: 0.01143\n",
      "[169,   100] loss: 0.01146\n",
      "[169,   120] loss: 0.01146\n",
      "[170,    20] loss: 0.01145\n",
      "[170,    40] loss: 0.01138\n",
      "[170,    60] loss: 0.01144\n",
      "[170,    80] loss: 0.01142\n",
      "[170,   100] loss: 0.01147\n",
      "[170,   120] loss: 0.01151\n",
      "[171,    20] loss: 0.01140\n",
      "[171,    40] loss: 0.01148\n",
      "[171,    60] loss: 0.01140\n",
      "[171,    80] loss: 0.01145\n",
      "[171,   100] loss: 0.01142\n",
      "[171,   120] loss: 0.01149\n",
      "[172,    20] loss: 0.01143\n",
      "[172,    40] loss: 0.01141\n",
      "[172,    60] loss: 0.01142\n",
      "[172,    80] loss: 0.01139\n",
      "[172,   100] loss: 0.01143\n",
      "[172,   120] loss: 0.01148\n",
      "[173,    20] loss: 0.01142\n",
      "[173,    40] loss: 0.01142\n",
      "[173,    60] loss: 0.01138\n",
      "[173,    80] loss: 0.01145\n",
      "[173,   100] loss: 0.01144\n",
      "[173,   120] loss: 0.01145\n",
      "[174,    20] loss: 0.01146\n",
      "[174,    40] loss: 0.01142\n",
      "[174,    60] loss: 0.01143\n",
      "[174,    80] loss: 0.01142\n",
      "[174,   100] loss: 0.01141\n",
      "[174,   120] loss: 0.01145\n",
      "[175,    20] loss: 0.01143\n",
      "[175,    40] loss: 0.01140\n",
      "[175,    60] loss: 0.01138\n",
      "[175,    80] loss: 0.01143\n",
      "[175,   100] loss: 0.01150\n",
      "[175,   120] loss: 0.01156\n",
      "[176,    20] loss: 0.01146\n",
      "[176,    40] loss: 0.01148\n",
      "[176,    60] loss: 0.01144\n",
      "[176,    80] loss: 0.01142\n",
      "[176,   100] loss: 0.01145\n",
      "[176,   120] loss: 0.01144\n",
      "[177,    20] loss: 0.01141\n",
      "[177,    40] loss: 0.01136\n",
      "[177,    60] loss: 0.01143\n",
      "[177,    80] loss: 0.01145\n",
      "[177,   100] loss: 0.01146\n",
      "[177,   120] loss: 0.01148\n",
      "[178,    20] loss: 0.01135\n",
      "[178,    40] loss: 0.01140\n",
      "[178,    60] loss: 0.01143\n",
      "[178,    80] loss: 0.01140\n",
      "[178,   100] loss: 0.01141\n",
      "[178,   120] loss: 0.01144\n",
      "[179,    20] loss: 0.01143\n",
      "[179,    40] loss: 0.01142\n",
      "[179,    60] loss: 0.01144\n",
      "[179,    80] loss: 0.01138\n",
      "[179,   100] loss: 0.01141\n",
      "[179,   120] loss: 0.01152\n",
      "[180,    20] loss: 0.01141\n",
      "[180,    40] loss: 0.01137\n",
      "[180,    60] loss: 0.01145\n",
      "[180,    80] loss: 0.01144\n",
      "[180,   100] loss: 0.01139\n",
      "[180,   120] loss: 0.01141\n",
      "[181,    20] loss: 0.01141\n",
      "[181,    40] loss: 0.01137\n",
      "[181,    60] loss: 0.01143\n",
      "[181,    80] loss: 0.01139\n",
      "[181,   100] loss: 0.01144\n",
      "[181,   120] loss: 0.01142\n",
      "[182,    20] loss: 0.01143\n",
      "[182,    40] loss: 0.01137\n",
      "[182,    60] loss: 0.01139\n",
      "[182,    80] loss: 0.01144\n",
      "[182,   100] loss: 0.01143\n",
      "[182,   120] loss: 0.01143\n",
      "[183,    20] loss: 0.01138\n",
      "[183,    40] loss: 0.01139\n",
      "[183,    60] loss: 0.01138\n",
      "[183,    80] loss: 0.01141\n",
      "[183,   100] loss: 0.01144\n",
      "[183,   120] loss: 0.01145\n",
      "[184,    20] loss: 0.01140\n",
      "[184,    40] loss: 0.01141\n",
      "[184,    60] loss: 0.01144\n",
      "[184,    80] loss: 0.01143\n",
      "[184,   100] loss: 0.01139\n",
      "[184,   120] loss: 0.01140\n",
      "[185,    20] loss: 0.01140\n",
      "[185,    40] loss: 0.01141\n",
      "[185,    60] loss: 0.01138\n",
      "[185,    80] loss: 0.01144\n",
      "[185,   100] loss: 0.01137\n",
      "[185,   120] loss: 0.01142\n",
      "[186,    20] loss: 0.01138\n",
      "[186,    40] loss: 0.01139\n",
      "[186,    60] loss: 0.01138\n",
      "[186,    80] loss: 0.01145\n",
      "[186,   100] loss: 0.01138\n",
      "[186,   120] loss: 0.01142\n",
      "[187,    20] loss: 0.01135\n",
      "[187,    40] loss: 0.01138\n",
      "[187,    60] loss: 0.01141\n",
      "[187,    80] loss: 0.01138\n",
      "[187,   100] loss: 0.01140\n",
      "[187,   120] loss: 0.01143\n",
      "[188,    20] loss: 0.01139\n",
      "[188,    40] loss: 0.01135\n",
      "[188,    60] loss: 0.01140\n",
      "[188,    80] loss: 0.01143\n",
      "[188,   100] loss: 0.01138\n",
      "[188,   120] loss: 0.01143\n",
      "[189,    20] loss: 0.01137\n",
      "[189,    40] loss: 0.01137\n",
      "[189,    60] loss: 0.01143\n",
      "[189,    80] loss: 0.01145\n",
      "[189,   100] loss: 0.01139\n",
      "[189,   120] loss: 0.01147\n",
      "[190,    20] loss: 0.01134\n",
      "[190,    40] loss: 0.01142\n",
      "[190,    60] loss: 0.01141\n",
      "[190,    80] loss: 0.01137\n",
      "[190,   100] loss: 0.01141\n",
      "[190,   120] loss: 0.01140\n",
      "[191,    20] loss: 0.01137\n",
      "[191,    40] loss: 0.01136\n",
      "[191,    60] loss: 0.01138\n",
      "[191,    80] loss: 0.01143\n",
      "[191,   100] loss: 0.01138\n",
      "[191,   120] loss: 0.01138\n",
      "[192,    20] loss: 0.01138\n",
      "[192,    40] loss: 0.01136\n",
      "[192,    60] loss: 0.01137\n",
      "[192,    80] loss: 0.01138\n",
      "[192,   100] loss: 0.01138\n",
      "[192,   120] loss: 0.01144\n",
      "[193,    20] loss: 0.01135\n",
      "[193,    40] loss: 0.01140\n",
      "[193,    60] loss: 0.01140\n",
      "[193,    80] loss: 0.01138\n",
      "[193,   100] loss: 0.01137\n",
      "[193,   120] loss: 0.01135\n",
      "[194,    20] loss: 0.01140\n",
      "[194,    40] loss: 0.01145\n",
      "[194,    60] loss: 0.01134\n",
      "[194,    80] loss: 0.01137\n",
      "[194,   100] loss: 0.01137\n",
      "[194,   120] loss: 0.01137\n",
      "[195,    20] loss: 0.01131\n",
      "[195,    40] loss: 0.01146\n",
      "[195,    60] loss: 0.01137\n",
      "[195,    80] loss: 0.01139\n",
      "[195,   100] loss: 0.01139\n",
      "[195,   120] loss: 0.01141\n",
      "[196,    20] loss: 0.01136\n",
      "[196,    40] loss: 0.01132\n",
      "[196,    60] loss: 0.01137\n",
      "[196,    80] loss: 0.01138\n",
      "[196,   100] loss: 0.01140\n",
      "[196,   120] loss: 0.01143\n",
      "[197,    20] loss: 0.01136\n",
      "[197,    40] loss: 0.01135\n",
      "[197,    60] loss: 0.01141\n",
      "[197,    80] loss: 0.01139\n",
      "[197,   100] loss: 0.01141\n",
      "[197,   120] loss: 0.01136\n",
      "[198,    20] loss: 0.01134\n",
      "[198,    40] loss: 0.01136\n",
      "[198,    60] loss: 0.01138\n",
      "[198,    80] loss: 0.01140\n",
      "[198,   100] loss: 0.01138\n",
      "[198,   120] loss: 0.01136\n",
      "[199,    20] loss: 0.01135\n",
      "[199,    40] loss: 0.01134\n",
      "[199,    60] loss: 0.01139\n",
      "[199,    80] loss: 0.01140\n",
      "[199,   100] loss: 0.01134\n",
      "[199,   120] loss: 0.01135\n",
      "[200,    20] loss: 0.01131\n",
      "[200,    40] loss: 0.01132\n",
      "[200,    60] loss: 0.01142\n",
      "[200,    80] loss: 0.01136\n",
      "[200,   100] loss: 0.01146\n",
      "[200,   120] loss: 0.01147\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DecoderTransformer(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(nums_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(\n",
    "            rearrange(outputs, 'batch seq vocab -> (batch seq) vocab'),\n",
    "            rearrange(labels, 'batch seq -> (batch seq)'),\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6, 10])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 10])\n",
      "torch.Size([64, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(outputs.transpose(1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 3, 8, 1, 9, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7, 7, 8, 8, 3, 8]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = t.randint(1,10, (1, 6))\n",
    "print(arr)\n",
    "model(arr).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 1, 1, 1, 8])\n",
      "tensor([8, 1, 1, 1, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "for x, y in nums_dl:\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
