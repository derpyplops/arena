{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fancy_einsum import einsum\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def singlehead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: t.Tensor):\n",
    "    '''\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    mask: shape (b, s, s)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    if mask is not None:\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def multihead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "    '''\n",
    "    Q: shape (b, s1, e)\n",
    "    K: shape (b, s2, e)\n",
    "    V: shape (b, s2, e)\n",
    "\n",
    "    e = nheads * h\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s e)\n",
    "    '''\n",
    "\n",
    "    # print(Q.shape[-1], n_heads, Q.shape[-1] // n_heads)\n",
    "    assert Q.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] % n_heads == 0\n",
    "    assert V.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "    # mask for autoencoder\n",
    "    mask = t.triu(t.ones(Q.shape[1], K.shape[1]), diagonal=1).bool()\n",
    "    # print(f'mask: {mask.shape}')\n",
    "\n",
    "    Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "    scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / math.sqrt(Q.shape[-1])\n",
    "    if mask is not None:\n",
    "        if mask.dim() == 2:\n",
    "            mask = repeat(mask, 's1 s2 -> b s1 s2', b=Q.shape[0])\n",
    "        else:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # print(mask.shape, scaled_dot_prod.shape)\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 1, -1e9)\n",
    "    attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "    attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "    \n",
    "    return rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        return self.W_O(multihead_attention(Q, K, V, self.num_heads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.2070, 15.0070, 15.8070],\n",
      "         [14.3999, 15.1999, 15.9999],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000]],\n",
      "\n",
      "        [[31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000]]])\n"
     ]
    }
   ],
   "source": [
    "# test single_head_attention\n",
    "Q = t.arange(2 * 7 * 3).reshape(2, 7, 3).type(t.float32)\n",
    "K = Q * 0.5\n",
    "V = Q * 0.8\n",
    "print(singlehead_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000, 14.6667, 14.3333, 14.0000],\n",
       "         [13.7668, 13.4335, 13.0346, 12.7012],\n",
       "         [12.3451, 12.0117, 11.6705, 11.3372],\n",
       "         [11.0013, 10.6679, 10.3337, 10.0004],\n",
       "         [ 9.6668,  9.3335,  9.0000,  8.6667]],\n",
       "\n",
       "        [[ 8.3333,  8.0000,  7.6667,  7.3333],\n",
       "         [ 7.0000,  6.6667,  6.3333,  6.0000],\n",
       "         [ 5.6667,  5.3333,  5.0000,  4.6667],\n",
       "         [ 4.3333,  4.0000,  3.6667,  3.3333],\n",
       "         [ 3.0000,  2.6667,  2.3333,  2.0000]]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = t.linspace(0, 10, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "K = t.linspace(5, 20, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "V = t.linspace(15, 2, 2 * 5 * 4).reshape(2, 5, 4)\n",
    "# b = 2, s = 5, c = 4\n",
    "multihead_attention(Q, K, V, n_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.7193,   0.4614,   0.4117,  -0.5813,   0.2754,  -0.5745],\n",
       "         [ -0.7746,   0.6206,   0.5520,  -0.7370,   0.1787,  -0.7289],\n",
       "         [ -1.1632,   1.7392,   1.5775,  -1.7907,  -0.5079,  -1.8103]],\n",
       "\n",
       "        [[  0.0549,  -1.9665, -10.8756,  -7.1792,   3.4559,   0.9521],\n",
       "         [ -0.3971,  -0.6652,  -9.6883,  -8.4108,   2.6582,  -0.3063],\n",
       "         [ -0.8686,   0.6920,  -8.4500,  -9.6953,   1.8262,  -1.6189]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = 'cpu'\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 6,\n",
    "    num_heads = 2,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        d = d_model\n",
    "        L = max_len\n",
    "        D = d / 2\n",
    "\n",
    "        angles = t.outer(t.arange(L), 1 / 10000 ** (2 * t.arange(D) / D))\n",
    "\n",
    "        array_2d = t.zeros((L, d))\n",
    "        array_2d[:, ::2] = t.sin(angles)\n",
    "        array_2d[:, 1::2] = t.cos(angles)\n",
    "        self.encoding = array_2d\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
    "        ''' \n",
    "        # print(x.shape)\n",
    "        batch_size, seq_len, embedding_dim = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        h1 = self.layernorm(self.attention(x) + x)\n",
    "        h2 = self.layernorm(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        decoders = [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size) # why? come back to this later\n",
    "        self.embed = lambda x: x# lambda tokens: tokens.unsqueeze(-1) # tokenizer does nothing at the moment\n",
    "        self.positional_embedding = PositionalEncoding(config.hidden_size)\n",
    "        self.unembed = lambda x: x #lambda x: x.squeeze # unembed does nothing at the moment\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedding = self.embed(tokens) # (seq_len) -> (seq_len, embedding)\n",
    "        pos_embedding = self.positional_embedding(tokens)\n",
    "        final_embedding = embedding + pos_embedding\n",
    "        a = self.dropout(final_embedding)\n",
    "        b = self.decoderlayer(a)\n",
    "        c = self.layernorm(b)\n",
    "        d = self.unembed(c)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    \"\"\"A toy dataset to train a model to reverse\n",
    "     a random sequence of tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = 25\n",
    "        self.total_size = 1000\n",
    "        self.text = t.rand((self.seq_len,\n",
    "                                config.hidden_size)).to(config.device).repeat(self.total_size,1,1)\n",
    "        # self.labels = t.rand((self.seq_len,\n",
    "        #                         config.hidden_size)).to(config.device).repeat(self.total_size,1,1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.text[idx,1:]\n",
    "        text = self.text[idx,:-1]\n",
    "        sample = {'text': text, 'label': label}\n",
    "        return sample\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "ds = TestDataSet(config)\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DecoderTransformer(config)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data.values()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
