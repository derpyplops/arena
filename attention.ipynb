{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fancy_einsum import einsum\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def singlehead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: t.Tensor):\n",
    "    '''\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    mask: shape (b, s, s)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    if mask is not None:\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def multihead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "    '''\n",
    "    Q: shape (b, s1, e)\n",
    "    K: shape (b, s2, e)\n",
    "    V: shape (b, s2, e)\n",
    "\n",
    "    e = nheads * h\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    h = hidden\n",
    "\n",
    "    Return: shape (b s e)\n",
    "    '''\n",
    "\n",
    "    assert Q.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] % n_heads == 0\n",
    "    assert V.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "    # mask for autoencoder\n",
    "    mask = t.triu(t.ones(Q.shape[1], K.shape[1]), diagonal=1).bool()\n",
    "\n",
    "    Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "    scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / math.sqrt(Q.shape[-1])\n",
    "    # if mask is not None:\n",
    "    #     if mask.dim() == 2:\n",
    "    #         mask = repeat(mask, 's1 s2 -> b s1 s2', b=Q.shape[0])\n",
    "    #     else:\n",
    "    #         mask = mask.unsqueeze(1)\n",
    "    #     # print(mask.shape, scaled_dot_prod.shape)\n",
    "    #     scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 1, -1e9)\n",
    "    mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "    scaled_dot_prod += mask_filter\n",
    "    attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "    attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "    attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "    return attention\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = multihead_attention(Q, K, V, self.num_heads)\n",
    "        return att # self.W_O(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.2070, 15.0070, 15.8070],\n",
      "         [14.3999, 15.1999, 15.9999],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000]],\n",
      "\n",
      "        [[31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000]]])\n"
     ]
    }
   ],
   "source": [
    "# test single_head_attention\n",
    "Q = t.arange(2 * 7 * 3).reshape(2, 7, 3).type(t.float32)\n",
    "K = Q * 0.5\n",
    "V = Q * 0.8\n",
    "print(singlehead_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000],\n",
       "         [14.5878],\n",
       "         [13.9747],\n",
       "         [13.2046],\n",
       "         [12.4225],\n",
       "         [11.6769],\n",
       "         [10.9564],\n",
       "         [10.2500],\n",
       "         [ 9.5519],\n",
       "         [ 8.8588]],\n",
       "\n",
       "        [[ 8.1579],\n",
       "         [ 7.4807],\n",
       "         [ 6.7942],\n",
       "         [ 6.1084],\n",
       "         [ 5.4231],\n",
       "         [ 4.7382],\n",
       "         [ 4.0535],\n",
       "         [ 3.3690],\n",
       "         [ 2.6846],\n",
       "         [ 2.0003]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = t.linspace(0, 10, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "K = t.linspace(5, 20, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "V = t.linspace(15, 2, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "# b = 2, s = 5, c = 4\n",
    "multihead_attention(Q, K, V, n_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.9540,   1.5843,   1.6350,  -1.2948,  -1.5780,  -2.8298],\n",
       "         [ -2.4249,   1.7430,   1.5237,  -1.3046,  -1.5791,  -2.8434],\n",
       "         [ -5.6502,   2.8301,   0.7615,  -1.2949,  -1.5780,  -2.8299]],\n",
       "\n",
       "        [[-13.5085,   5.4788,  -1.0956, -18.9775,  -3.4824, -27.3478],\n",
       "         [-17.2725,   6.7475,  -1.9851, -18.9775,  -3.4824, -27.3478],\n",
       "         [-21.1983,   8.0707,  -2.9128, -18.9775,  -3.4824, -27.3478]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = 'cpu'\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 6,\n",
    "    num_heads = 4,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        d = d_model\n",
    "        L = max_len\n",
    "        D = d / 2\n",
    "\n",
    "        angles = t.outer(t.arange(L), 1 / 10000 ** (2 * t.arange(D) / D))\n",
    "\n",
    "        array_2d = t.zeros((L, d))\n",
    "        array_2d[:, ::2] = t.sin(angles)\n",
    "        array_2d[:, 1::2] = t.cos(angles)\n",
    "        self.encoding = array_2d\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
    "        ''' \n",
    "        batch_size, seq_len, embedding_dim = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        att = self.attention(x) + x\n",
    "        h1 = self.layernorm(att)\n",
    "        h2 = self.layernorm(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        decoders = [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size) # why? come back to this later\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_embedding = PositionalEncoding(config.hidden_size)\n",
    "        self.last_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        embedding = self.embed(tokens) # (seq_len) -> (seq_len, embedding)\n",
    "        pos_embedding = self.positional_embedding(embedding)\n",
    "        final_embedding = embedding + pos_embedding\n",
    "        a = self.dropout(final_embedding)\n",
    "        b = self.decoderlayer(a)\n",
    "        c = self.layernorm(b)\n",
    "        d = self.last_linear(c).log_softmax(dim=-1)\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    \"\"\"A toy dataset to train a model to predict\n",
    "     a random sequence of tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = 25\n",
    "        self.total_size = 1000\n",
    "        self.text = t.randint(0,config.vocab_size, (self.total_size, self.seq_len)).to(config.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.text[idx,1:]\n",
    "        text = self.text[idx,:-1]\n",
    "        return (text, label)\n",
    "\n",
    "class ReversedNumbers(Dataset):\n",
    "    def __init__(self, vocab_size: int, seq_len: int, datasize: int):\n",
    "        self.seqs = t.randint(0, vocab_size, (datasize, seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            input = self.seqs[idx]\n",
    "            target = t.flip(input, dims=(0,))\n",
    "            return (input, target)\n",
    "\n",
    "# class ShakespeareDataset(Dataset):\n",
    "#     def __init__(self, config):\n",
    "#         self.data = open('shakespeare.txt', 'r').read()\n",
    "#         self.config = config\n",
    "#         chars = sorted(set(self.data))\n",
    "#         self.vocab_size = len(chars)\n",
    "#         self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "#         self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#         print('data has %d characters, %d unique.' % (len(self.data), self.vocab_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.char_to_idx[self.data[index]]\n",
    "#         x = t.tensor([x])\n",
    "#         x = F.one_hot(x, num_classes=self.vocab_size)\n",
    "#         x = x.type(t.FloatTensor)\n",
    "#         t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n",
    "#         t = t.tensor([t])\n",
    "#         return (x.to(self.config.device), t.to(self.config.device))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def params(self):\n",
    "#         return self.vocab_size, self.char_to_idx, self.idx_to_char\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "dummy_ds = TestDataSet(config)\n",
    "dummy_dl = DataLoader(dummy_ds, batch_size=2, shuffle=True)\n",
    "nums_ds = ReversedNumbers(vocab_size=10, seq_len=10, datasize=1000)\n",
    "train_ds, val_ds = random_split(nums_ds, [800, 200])\n",
    "nums_dl = DataLoader(train_ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.23680\n",
      "[1,   400] loss: 0.23307\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DecoderTransformer(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dummy_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.transpose(1,2), labels.long())\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
