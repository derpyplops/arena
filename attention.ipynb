{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fancy_einsum import einsum\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def singlehead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: t.Tensor):\n",
    "    '''\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    mask: shape (b, s, s)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    if mask is not None:\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def multihead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "    '''\n",
    "    Q: shape (b, s1, e)\n",
    "    K: shape (b, s2, e)\n",
    "    V: shape (b, s2, e)\n",
    "\n",
    "    e = nheads * h\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    h = hidden\n",
    "\n",
    "    Return: shape (b s e)\n",
    "    '''\n",
    "\n",
    "    assert Q.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] % n_heads == 0\n",
    "    assert V.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "    # mask for autoencoder\n",
    "    mask = t.triu(t.ones(Q.shape[1], K.shape[1]), diagonal=1).bool()\n",
    "\n",
    "    Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "    scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / math.sqrt(Q.shape[-1])\n",
    "    # if mask is not None:\n",
    "    #     if mask.dim() == 2:\n",
    "    #         mask = repeat(mask, 's1 s2 -> b s1 s2', b=Q.shape[0])\n",
    "    #     else:\n",
    "    #         mask = mask.unsqueeze(1)\n",
    "    #     # print(mask.shape, scaled_dot_prod.shape)\n",
    "    #     scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 1, -1e9)\n",
    "    mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "    scaled_dot_prod += mask_filter\n",
    "    attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "    attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "    attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "    return attention\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = multihead_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.2070, 15.0070, 15.8070],\n",
      "         [14.3999, 15.1999, 15.9999],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000]],\n",
      "\n",
      "        [[31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000]]])\n"
     ]
    }
   ],
   "source": [
    "# test single_head_attention\n",
    "Q = t.arange(2 * 7 * 3).reshape(2, 7, 3).type(t.float32)\n",
    "K = Q * 0.5\n",
    "V = Q * 0.8\n",
    "print(singlehead_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000],\n",
       "         [14.5878],\n",
       "         [13.9747],\n",
       "         [13.2046],\n",
       "         [12.4225],\n",
       "         [11.6769],\n",
       "         [10.9564],\n",
       "         [10.2500],\n",
       "         [ 9.5519],\n",
       "         [ 8.8588]],\n",
       "\n",
       "        [[ 8.1579],\n",
       "         [ 7.4807],\n",
       "         [ 6.7942],\n",
       "         [ 6.1084],\n",
       "         [ 5.4231],\n",
       "         [ 4.7382],\n",
       "         [ 4.0535],\n",
       "         [ 3.3690],\n",
       "         [ 2.6846],\n",
       "         [ 2.0003]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = t.linspace(0, 10, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "K = t.linspace(5, 20, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "V = t.linspace(15, 2, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "# b = 2, s = 5, c = 4\n",
    "multihead_attention(Q, K, V, n_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.7193,   0.4614,   0.4117,  -0.5813,   0.2754,  -0.5745],\n",
       "         [ -0.7746,   0.6206,   0.5520,  -0.7370,   0.1787,  -0.7289],\n",
       "         [ -1.1632,   1.7392,   1.5775,  -1.7907,  -0.5079,  -1.8103]],\n",
       "\n",
       "        [[  0.0549,  -1.9665, -10.8756,  -7.1792,   3.4559,   0.9521],\n",
       "         [ -0.3971,  -0.6652,  -9.6883,  -8.4108,   2.6582,  -0.3063],\n",
       "         [ -0.8686,   0.6920,  -8.4500,  -9.6953,   1.8262,  -1.6189]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = 'cpu'\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 6,\n",
    "    num_heads = 4,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        d = d_model\n",
    "        L = max_len\n",
    "        D = d / 2\n",
    "\n",
    "        angles = t.outer(t.arange(L), 1 / 10000 ** (2 * t.arange(D) / D))\n",
    "\n",
    "        array_2d = t.zeros((L, d))\n",
    "        array_2d[:, ::2] = t.sin(angles)\n",
    "        array_2d[:, 1::2] = t.cos(angles)\n",
    "        self.encoding = array_2d\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
    "        ''' \n",
    "        batch_size, seq_len, embedding_dim = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "#     def __init__(self, d_in: int, d_out: int):\n",
    "#         super().__init__()\n",
    "#         d_h = d_in * 4\n",
    "#         self.model = nn.Sequential(OrderedDict([\n",
    "#             ('linear1', nn.Linear(d_in, d_h)),\n",
    "#             ('GELU', nn.GELU()),\n",
    "#             ('linear2', nn.Linear(d_h, d_in)),   \n",
    "#             ('dropout', nn.Dropout(p=0.1))\n",
    "#         ]))\n",
    "\n",
    "#     def forward(self, x: t.Tensor):\n",
    "#         return self.model(x)\n",
    "        \n",
    "# class DecoderBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, config: TransformerConfig):\n",
    "#         super().__init__()\n",
    "#         self.attention = MultiheadMaskedAttention(\n",
    "#             hidden_size=config.hidden_size,\n",
    "#             num_heads=config.num_heads\n",
    "#         )\n",
    "#         self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "#         self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "#         self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "#     def forward(self, x: t.Tensor):\n",
    "#         att = self.attention(x) + x\n",
    "#         h1 = self.layernorm1(att)\n",
    "#         h2 = self.layernorm2(self.mlp(h1) + h1)\n",
    "#         return h2\n",
    "\n",
    "# class DecoderTransformer(nn.Module):\n",
    "\n",
    "#     def __init__(self, config: TransformerConfig):\n",
    "#         super().__init__()\n",
    "#         decoders = [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "#         names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "#         self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "#         self.dropout = nn.Dropout(p=config.dropout)\n",
    "#         self.layernorm = nn.LayerNorm(config.hidden_size) # why? come back to this later\n",
    "#         self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "#         self.positional_embedding = PositionalEncoding(config.hidden_size)\n",
    "#         self.last_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "#     def forward(self, tokens):\n",
    "#         embedding = self.embed(tokens) # (seq_len) -> (seq_len, embedding)\n",
    "#         pos_embedding = self.positional_embedding(embedding)\n",
    "#         final_embedding = embedding + pos_embedding\n",
    "#         a = self.dropout(final_embedding)\n",
    "#         b = self.decoderlayer(a)\n",
    "#         c = self.layernorm(b) @ self.embed.weight.T\n",
    "#         return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    \"\"\"A toy dataset to train a model to predict\n",
    "     a random sequence of tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = 25\n",
    "        self.total_size = 1000\n",
    "        self.text = t.randint(0,config.vocab_size, (self.total_size, self.seq_len)).to(config.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.text[idx,1:]\n",
    "        text = self.text[idx,:-1]\n",
    "        return (text, label)\n",
    "\n",
    "class ReversedNumbers(Dataset):\n",
    "    def __init__(self, vocab_size: int, seq_len: int, datasize: int):\n",
    "        self.seqs = t.randint(0, vocab_size, (datasize, seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            input = self.seqs[idx]\n",
    "            target = t.flip(input, dims=(0,))\n",
    "            return (input, target)\n",
    "\n",
    "# class ShakespeareDataset(Dataset):\n",
    "#     def __init__(self, config):\n",
    "#         self.data = open('shakespeare.txt', 'r').read()\n",
    "#         self.config = config\n",
    "#         chars = sorted(set(self.data))\n",
    "#         self.vocab_size = len(chars)\n",
    "#         self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "#         self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#         print('data has %d characters, %d unique.' % (len(self.data), self.vocab_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.char_to_idx[self.data[index]]\n",
    "#         x = t.tensor([x])\n",
    "#         x = F.one_hot(x, num_classes=self.vocab_size)\n",
    "#         x = x.type(t.FloatTensor)\n",
    "#         t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n",
    "#         t = t.tensor([t])\n",
    "#         return (x.to(self.config.device), t.to(self.config.device))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def params(self):\n",
    "#         return self.vocab_size, self.char_to_idx, self.idx_to_char\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "# dummy_ds = TestDataSet(config)\n",
    "# dummy_dl = DataLoader(dummy_ds, batch_size=64, shuffle=True)\n",
    "nums_ds = ReversedNumbers(vocab_size=10, seq_len=6, datasize=10000)\n",
    "train_ds, val_ds = random_split(nums_ds, [8000, 2000])\n",
    "nums_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "nums_tl = DataLoader(val_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.25672\n",
      "[1,    40] loss: 0.04750\n",
      "[1,    60] loss: 0.03223\n",
      "[1,    80] loss: 0.03015\n",
      "[1,   100] loss: 0.02905\n",
      "[1,   120] loss: 0.02844\n",
      "accuracy: 0.098\n",
      "[2,    20] loss: 0.02738\n",
      "[2,    40] loss: 0.02679\n",
      "[2,    60] loss: 0.02639\n",
      "[2,    80] loss: 0.02627\n",
      "[2,   100] loss: 0.02572\n",
      "[2,   120] loss: 0.02523\n",
      "accuracy: 0.100\n",
      "[3,    20] loss: 0.02509\n",
      "[3,    40] loss: 0.02483\n",
      "[3,    60] loss: 0.02459\n",
      "[3,    80] loss: 0.02451\n",
      "[3,   100] loss: 0.02433\n",
      "[3,   120] loss: 0.02413\n",
      "accuracy: 0.113\n",
      "[4,    20] loss: 0.02397\n",
      "[4,    40] loss: 0.02402\n",
      "[4,    60] loss: 0.02398\n",
      "[4,    80] loss: 0.02380\n",
      "[4,   100] loss: 0.02377\n",
      "[4,   120] loss: 0.02358\n",
      "accuracy: 0.123\n",
      "[5,    20] loss: 0.02349\n",
      "[5,    40] loss: 0.02344\n",
      "[5,    60] loss: 0.02333\n",
      "[5,    80] loss: 0.02330\n",
      "[5,   100] loss: 0.02333\n",
      "[5,   120] loss: 0.02321\n",
      "accuracy: 0.134\n",
      "[6,    20] loss: 0.02311\n",
      "[6,    40] loss: 0.02304\n",
      "[6,    60] loss: 0.02301\n",
      "[6,    80] loss: 0.02286\n",
      "[6,   100] loss: 0.02285\n",
      "[6,   120] loss: 0.02288\n",
      "accuracy: 0.174\n",
      "[7,    20] loss: 0.02281\n",
      "[7,    40] loss: 0.02261\n",
      "[7,    60] loss: 0.02266\n",
      "[7,    80] loss: 0.02270\n",
      "[7,   100] loss: 0.02248\n",
      "[7,   120] loss: 0.02240\n",
      "accuracy: 0.199\n",
      "[8,    20] loss: 0.02231\n",
      "[8,    40] loss: 0.02227\n",
      "[8,    60] loss: 0.02234\n",
      "[8,    80] loss: 0.02235\n",
      "[8,   100] loss: 0.02222\n",
      "[8,   120] loss: 0.02207\n",
      "accuracy: 0.217\n",
      "[9,    20] loss: 0.02211\n",
      "[9,    40] loss: 0.02202\n",
      "[9,    60] loss: 0.02196\n",
      "[9,    80] loss: 0.02192\n",
      "[9,   100] loss: 0.02197\n",
      "[9,   120] loss: 0.02181\n",
      "accuracy: 0.215\n",
      "[10,    20] loss: 0.02177\n",
      "[10,    40] loss: 0.02172\n",
      "[10,    60] loss: 0.02166\n",
      "[10,    80] loss: 0.02162\n",
      "[10,   100] loss: 0.02147\n",
      "[10,   120] loss: 0.02153\n",
      "accuracy: 0.236\n",
      "[11,    20] loss: 0.02139\n",
      "[11,    40] loss: 0.02129\n",
      "[11,    60] loss: 0.02111\n",
      "[11,    80] loss: 0.02097\n",
      "[11,   100] loss: 0.02113\n",
      "[11,   120] loss: 0.02103\n",
      "accuracy: 0.239\n",
      "[12,    20] loss: 0.02081\n",
      "[12,    40] loss: 0.02076\n",
      "[12,    60] loss: 0.02082\n",
      "[12,    80] loss: 0.02082\n",
      "[12,   100] loss: 0.02070\n",
      "[12,   120] loss: 0.02049\n",
      "accuracy: 0.240\n",
      "[13,    20] loss: 0.02056\n",
      "[13,    40] loss: 0.02051\n",
      "[13,    60] loss: 0.02038\n",
      "[13,    80] loss: 0.02025\n",
      "[13,   100] loss: 0.02022\n",
      "[13,   120] loss: 0.02007\n",
      "accuracy: 0.248\n",
      "[14,    20] loss: 0.01994\n",
      "[14,    40] loss: 0.01990\n",
      "[14,    60] loss: 0.01980\n",
      "[14,    80] loss: 0.01999\n",
      "[14,   100] loss: 0.01982\n",
      "[14,   120] loss: 0.01957\n",
      "accuracy: 0.251\n",
      "[15,    20] loss: 0.01942\n",
      "[15,    40] loss: 0.01937\n",
      "[15,    60] loss: 0.01925\n",
      "[15,    80] loss: 0.01936\n",
      "[15,   100] loss: 0.01922\n",
      "[15,   120] loss: 0.01903\n",
      "accuracy: 0.254\n",
      "[16,    20] loss: 0.01886\n",
      "[16,    40] loss: 0.01902\n",
      "[16,    60] loss: 0.01881\n",
      "[16,    80] loss: 0.01875\n",
      "[16,   100] loss: 0.01859\n",
      "[16,   120] loss: 0.01849\n",
      "accuracy: 0.268\n",
      "[17,    20] loss: 0.01842\n",
      "[17,    40] loss: 0.01825\n",
      "[17,    60] loss: 0.01821\n",
      "[17,    80] loss: 0.01821\n",
      "[17,   100] loss: 0.01802\n",
      "[17,   120] loss: 0.01788\n",
      "accuracy: 0.289\n",
      "[18,    20] loss: 0.01784\n",
      "[18,    40] loss: 0.01778\n",
      "[18,    60] loss: 0.01756\n",
      "[18,    80] loss: 0.01739\n",
      "[18,   100] loss: 0.01738\n",
      "[18,   120] loss: 0.01721\n",
      "accuracy: 0.329\n",
      "[19,    20] loss: 0.01712\n",
      "[19,    40] loss: 0.01691\n",
      "[19,    60] loss: 0.01676\n",
      "[19,    80] loss: 0.01673\n",
      "[19,   100] loss: 0.01655\n",
      "[19,   120] loss: 0.01646\n",
      "accuracy: 0.366\n",
      "[20,    20] loss: 0.01615\n",
      "[20,    40] loss: 0.01604\n",
      "[20,    60] loss: 0.01586\n",
      "[20,    80] loss: 0.01573\n",
      "[20,   100] loss: 0.01556\n",
      "[20,   120] loss: 0.01561\n",
      "accuracy: 0.417\n",
      "[21,    20] loss: 0.01534\n",
      "[21,    40] loss: 0.01535\n",
      "[21,    60] loss: 0.01520\n",
      "[21,    80] loss: 0.01514\n",
      "[21,   100] loss: 0.01501\n",
      "[21,   120] loss: 0.01494\n",
      "accuracy: 0.432\n",
      "[22,    20] loss: 0.01469\n",
      "[22,    40] loss: 0.01458\n",
      "[22,    60] loss: 0.01478\n",
      "[22,    80] loss: 0.01462\n",
      "[22,   100] loss: 0.01453\n",
      "[22,   120] loss: 0.01454\n",
      "accuracy: 0.458\n",
      "[23,    20] loss: 0.01462\n",
      "[23,    40] loss: 0.01440\n",
      "[23,    60] loss: 0.01429\n",
      "[23,    80] loss: 0.01425\n",
      "[23,   100] loss: 0.01424\n",
      "[23,   120] loss: 0.01424\n",
      "accuracy: 0.471\n",
      "[24,    20] loss: 0.01421\n",
      "[24,    40] loss: 0.01416\n",
      "[24,    60] loss: 0.01399\n",
      "[24,    80] loss: 0.01412\n",
      "[24,   100] loss: 0.01410\n",
      "[24,   120] loss: 0.01394\n",
      "accuracy: 0.478\n",
      "[25,    20] loss: 0.01388\n",
      "[25,    40] loss: 0.01384\n",
      "[25,    60] loss: 0.01404\n",
      "[25,    80] loss: 0.01391\n",
      "[25,   100] loss: 0.01382\n",
      "[25,   120] loss: 0.01374\n",
      "accuracy: 0.480\n",
      "[26,    20] loss: 0.01393\n",
      "[26,    40] loss: 0.01379\n",
      "[26,    60] loss: 0.01372\n",
      "[26,    80] loss: 0.01354\n",
      "[26,   100] loss: 0.01360\n",
      "[26,   120] loss: 0.01370\n",
      "accuracy: 0.487\n",
      "[27,    20] loss: 0.01365\n",
      "[27,    40] loss: 0.01361\n",
      "[27,    60] loss: 0.01377\n",
      "[27,    80] loss: 0.01384\n",
      "[27,   100] loss: 0.01346\n",
      "[27,   120] loss: 0.01358\n",
      "accuracy: 0.488\n",
      "[28,    20] loss: 0.01346\n",
      "[28,    40] loss: 0.01362\n",
      "[28,    60] loss: 0.01348\n",
      "[28,    80] loss: 0.01343\n",
      "[28,   100] loss: 0.01353\n",
      "[28,   120] loss: 0.01350\n",
      "accuracy: 0.497\n",
      "[29,    20] loss: 0.01326\n",
      "[29,    40] loss: 0.01328\n",
      "[29,    60] loss: 0.01333\n",
      "[29,    80] loss: 0.01335\n",
      "[29,   100] loss: 0.01339\n",
      "[29,   120] loss: 0.01332\n",
      "accuracy: 0.496\n",
      "[30,    20] loss: 0.01328\n",
      "[30,    40] loss: 0.01324\n",
      "[30,    60] loss: 0.01327\n",
      "[30,    80] loss: 0.01338\n",
      "[30,   100] loss: 0.01319\n",
      "[30,   120] loss: 0.01325\n",
      "accuracy: 0.493\n",
      "[31,    20] loss: 0.01332\n",
      "[31,    40] loss: 0.01322\n",
      "[31,    60] loss: 0.01329\n",
      "[31,    80] loss: 0.01330\n",
      "[31,   100] loss: 0.01337\n",
      "[31,   120] loss: 0.01311\n",
      "accuracy: 0.504\n",
      "[32,    20] loss: 0.01308\n",
      "[32,    40] loss: 0.01307\n",
      "[32,    60] loss: 0.01306\n",
      "[32,    80] loss: 0.01308\n",
      "[32,   100] loss: 0.01302\n",
      "[32,   120] loss: 0.01297\n",
      "accuracy: 0.506\n",
      "[33,    20] loss: 0.01322\n",
      "[33,    40] loss: 0.01302\n",
      "[33,    60] loss: 0.01297\n",
      "[33,    80] loss: 0.01314\n",
      "[33,   100] loss: 0.01311\n",
      "[33,   120] loss: 0.01309\n",
      "accuracy: 0.504\n",
      "[34,    20] loss: 0.01303\n",
      "[34,    40] loss: 0.01306\n",
      "[34,    60] loss: 0.01293\n",
      "[34,    80] loss: 0.01273\n",
      "[34,   100] loss: 0.01302\n",
      "[34,   120] loss: 0.01296\n",
      "accuracy: 0.508\n",
      "[35,    20] loss: 0.01291\n",
      "[35,    40] loss: 0.01294\n",
      "[35,    60] loss: 0.01298\n",
      "[35,    80] loss: 0.01298\n",
      "[35,   100] loss: 0.01283\n",
      "[35,   120] loss: 0.01303\n",
      "accuracy: 0.515\n",
      "[36,    20] loss: 0.01294\n",
      "[36,    40] loss: 0.01285\n",
      "[36,    60] loss: 0.01285\n",
      "[36,    80] loss: 0.01270\n",
      "[36,   100] loss: 0.01281\n",
      "[36,   120] loss: 0.01286\n",
      "accuracy: 0.511\n",
      "[37,    20] loss: 0.01280\n",
      "[37,    40] loss: 0.01279\n",
      "[37,    60] loss: 0.01273\n",
      "[37,    80] loss: 0.01285\n",
      "[37,   100] loss: 0.01269\n",
      "[37,   120] loss: 0.01278\n",
      "accuracy: 0.513\n",
      "[38,    20] loss: 0.01280\n",
      "[38,    40] loss: 0.01273\n",
      "[38,    60] loss: 0.01272\n",
      "[38,    80] loss: 0.01273\n",
      "[38,   100] loss: 0.01261\n",
      "[38,   120] loss: 0.01268\n",
      "accuracy: 0.515\n",
      "[39,    20] loss: 0.01275\n",
      "[39,    40] loss: 0.01275\n",
      "[39,    60] loss: 0.01268\n",
      "[39,    80] loss: 0.01265\n",
      "[39,   100] loss: 0.01271\n",
      "[39,   120] loss: 0.01264\n",
      "accuracy: 0.522\n",
      "[40,    20] loss: 0.01259\n",
      "[40,    40] loss: 0.01279\n",
      "[40,    60] loss: 0.01258\n",
      "[40,    80] loss: 0.01259\n",
      "[40,   100] loss: 0.01272\n",
      "[40,   120] loss: 0.01268\n",
      "accuracy: 0.521\n",
      "[41,    20] loss: 0.01256\n",
      "[41,    40] loss: 0.01263\n",
      "[41,    60] loss: 0.01272\n",
      "[41,    80] loss: 0.01261\n",
      "[41,   100] loss: 0.01258\n",
      "[41,   120] loss: 0.01255\n",
      "accuracy: 0.517\n",
      "[42,    20] loss: 0.01248\n",
      "[42,    40] loss: 0.01258\n",
      "[42,    60] loss: 0.01260\n",
      "[42,    80] loss: 0.01249\n",
      "[42,   100] loss: 0.01248\n",
      "[42,   120] loss: 0.01240\n",
      "accuracy: 0.520\n",
      "[43,    20] loss: 0.01254\n",
      "[43,    40] loss: 0.01244\n",
      "[43,    60] loss: 0.01243\n",
      "[43,    80] loss: 0.01247\n",
      "[43,   100] loss: 0.01243\n",
      "[43,   120] loss: 0.01244\n",
      "accuracy: 0.525\n",
      "[44,    20] loss: 0.01246\n",
      "[44,    40] loss: 0.01256\n",
      "[44,    60] loss: 0.01251\n",
      "[44,    80] loss: 0.01245\n",
      "[44,   100] loss: 0.01249\n",
      "[44,   120] loss: 0.01251\n",
      "accuracy: 0.519\n",
      "[45,    20] loss: 0.01249\n",
      "[45,    40] loss: 0.01244\n",
      "[45,    60] loss: 0.01250\n",
      "[45,    80] loss: 0.01256\n",
      "[45,   100] loss: 0.01237\n",
      "[45,   120] loss: 0.01241\n",
      "accuracy: 0.528\n",
      "[46,    20] loss: 0.01240\n",
      "[46,    40] loss: 0.01229\n",
      "[46,    60] loss: 0.01235\n",
      "[46,    80] loss: 0.01224\n",
      "[46,   100] loss: 0.01227\n",
      "[46,   120] loss: 0.01237\n",
      "accuracy: 0.526\n",
      "[47,    20] loss: 0.01236\n",
      "[47,    40] loss: 0.01236\n",
      "[47,    60] loss: 0.01236\n",
      "[47,    80] loss: 0.01217\n",
      "[47,   100] loss: 0.01246\n",
      "[47,   120] loss: 0.01244\n",
      "accuracy: 0.529\n",
      "[48,    20] loss: 0.01233\n",
      "[48,    40] loss: 0.01224\n",
      "[48,    60] loss: 0.01226\n",
      "[48,    80] loss: 0.01234\n",
      "[48,   100] loss: 0.01231\n",
      "[48,   120] loss: 0.01221\n",
      "accuracy: 0.536\n",
      "[49,    20] loss: 0.01233\n",
      "[49,    40] loss: 0.01232\n",
      "[49,    60] loss: 0.01224\n",
      "[49,    80] loss: 0.01217\n",
      "[49,   100] loss: 0.01215\n",
      "[49,   120] loss: 0.01223\n",
      "accuracy: 0.529\n",
      "[50,    20] loss: 0.01230\n",
      "[50,    40] loss: 0.01222\n",
      "[50,    60] loss: 0.01218\n",
      "[50,    80] loss: 0.01223\n",
      "[50,   100] loss: 0.01230\n",
      "[50,   120] loss: 0.01231\n",
      "accuracy: 0.528\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from impl.transformer_modules import DecoderTransformer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DecoderTransformer(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(nums_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(\n",
    "            rearrange(outputs, 'batch seq vocab -> batch vocab seq'),\n",
    "            labels\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "        \n",
    "        \n",
    "    for (x, y) in nums_tl:\n",
    "\n",
    "        x = x.to(config.device)\n",
    "        y = y.to(config.device)\n",
    "\n",
    "        y_hat = model(x)\n",
    "        y_predictions = y_hat.argmax(2)\n",
    "        accuracy += (y_predictions == y).sum().item()\n",
    "        total += y.size(0) * 6\n",
    "\n",
    "        accuracy_list.append(accuracy/total)\n",
    "    print(f'accuracy: {accuracy/total:.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6, 10])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 10])\n",
      "torch.Size([64, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(outputs.transpose(1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 2, 5, 7, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8, 4, 9, 2, 3, 1]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = t.randint(1,10, (1, 6))\n",
    "print(arr)\n",
    "model(arr).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 9, 8, 5, 5, 6])\n",
      "tensor([6, 5, 5, 8, 9, 7])\n"
     ]
    }
   ],
   "source": [
    "for x, y in nums_dl:\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
