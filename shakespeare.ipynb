{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the complete works of shakespeare as a text file and save it in the home directory\n",
    "# !wget https://www.gutenberg.org/files/100/100-0.txt -O ./shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/derpyplops/arena/blob/main/shakespeare.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: bad pattern: [Open\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from fancy_einsum import einsum\n",
    "from torch import optim\n",
    "from impl.transformer_modules import DecoderTransformer, TransformerConfig\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Complete', ' ', 'Works', ' ', 'of', ' ', 'William', ' ', 'Shakespeare', ', ', 'by', ' ', 'William', ' ', 'Shakespeare', '\\n\\n', 'This', ' ', 'eBook', ' ', 'is', ' ', 'for', ' ', 'the', ' ', 'use', ' ', 'of', ' ', 'anyone', ' ', 'anywhere', ' ', 'in', ' ', 'the', ' ', 'United', ' ', 'States', ' ', 'and', '\\n', 'most', ' ', 'other', ' ', 'parts', ' ', 'of', ' ', 'the', ' ', 'world', ' ', 'at', ' ', 'no', ' ', 'cost', ' ', 'and', ' ', 'with', ' ', 'almost', ' ', 'no', ' ', 'restrictions', '\\n', 'whatsoever', '. ', 'You', ' ', 'may', ' ', 'copy', ' ', 'it', ', ', 'give', ' ', 'it', ' ', 'away']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# read the file\n",
    "with open('./shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(re.split(r\"\\b\", text)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset containing shakespeare\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import re\n",
    "\n",
    "# read the file\n",
    "with open('./shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.vocab = sorted(set(text))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
    "        self.text_as_int = t.tensor([self.char_to_idx[c] for c in self.text])\n",
    "\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "        self.num_batches = int(len(text) / (seq_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_as_int[idx * self.seq_size:(idx + 1) * self.seq_size]\n",
    "        label = self.text_as_int[idx * self.seq_size + 1:(idx + 1) * self.seq_size + 1]\n",
    "        return (text, label)\n",
    "\n",
    "    def to_text(self, idxs):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in idxs])\n",
    "\n",
    "    def to_int(self, text):\n",
    "        return [self.char_to_idx[c] for c in text]\n",
    "\n",
    "    def to_one_hot(self, idxs):\n",
    "        return t.eye(self.vocab_size)[idxs]\n",
    "\n",
    "    def to_text_from_one_hot(self, one_hot):\n",
    "        return self.to_text(t.argmax(one_hot, dim=-1))\n",
    "\n",
    "# create the dataset\n",
    "shakespeare_dataset = ShakespeareDataset(re.split(r\"\\b\", text), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([34542,  9992,   113,  8237,   113,  5523,   113, 17830,   113, 24979,\n",
      "          113,  9992,   113,  3477,   113, 10995,   113, 24979,   113, 10916,\n",
      "          113,  9165,   480, 14228,   113, 10916,   113,  9165,     1, 10039,\n",
      "          113, 17830,   113, 22293,   113, 19582,   113, 31392,   113, 33037,\n",
      "          113, 24979,   113, 12315,   113, 12317,   113, 21768,   113, 31392,\n",
      "          113, 10403,   113,  9566,   113, 12244,     0, 24317,   113, 25174,\n",
      "          113, 25577,   113, 24979,   113, 31392,   113, 34221,   113, 12640,\n",
      "          113, 24717,   113, 15883,   113, 12244,   113, 34099,   113, 12140,\n",
      "          113, 24717,   113, 28036,     0, 33837,   786, 11076,   113, 23721,\n",
      "          113, 15820,   113, 22310,   480, 20265,   113, 22310,   113, 12779])\n",
      "tensor([ 9992,   113,  8237,   113,  5523,   113, 17830,   113, 24979,   113,\n",
      "         9992,   113,  3477,   113, 10995,   113, 24979,   113, 10916,   113,\n",
      "         9165,   480, 14228,   113, 10916,   113,  9165,     1, 10039,   113,\n",
      "        17830,   113, 22293,   113, 19582,   113, 31392,   113, 33037,   113,\n",
      "        24979,   113, 12315,   113, 12317,   113, 21768,   113, 31392,   113,\n",
      "        10403,   113,  9566,   113, 12244,     0, 24317,   113, 25174,   113,\n",
      "        25577,   113, 24979,   113, 31392,   113, 34221,   113, 12640,   113,\n",
      "        24717,   113, 15883,   113, 12244,   113, 34099,   113, 12140,   113,\n",
      "        24717,   113, 28036,     0, 33837,   786, 11076,   113, 23721,   113,\n",
      "        15820,   113, 22310,   480, 20265,   113, 22310,   113, 12779,   113])\n"
     ]
    }
   ],
   "source": [
    "# print(shakespeare_dataset.text[0:52])\n",
    "# print(shakespeare_dataset.vocab)\n",
    "\n",
    "for x, y in shakespeare_dataset:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TransformerConfig, model, train_dataloader: DataLoader, optimizer, criterion):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = DecoderTransformer(config)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in range(3):  # loop over the dataset multiple times\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(\n",
    "                rearrange(outputs, 'batch seq vocab -> batch vocab seq'),\n",
    "                labels\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            print(i)\n",
    "            # if i % 20 == 19:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "            \n",
    "            \n",
    "        # for (x, y) in train_dataloader:\n",
    "\n",
    "        #     x = x.to(config.device)\n",
    "        #     y = y.to(config.device)\n",
    "\n",
    "        #     y_hat = model(x)\n",
    "        #     y_predictions = y_hat.argmax(2)\n",
    "        #     accuracy += (y_predictions == y).sum().item()\n",
    "        #     total += y.size(0) * 6\n",
    "\n",
    "        #     accuracy_list.append(accuracy/total)\n",
    "        # print(f'accuracy: {accuracy/total:.3f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1,     1] loss: 0.09867\n",
      "1\n",
      "[1,     2] loss: 0.09457\n",
      "2\n",
      "[1,     3] loss: 0.08988\n",
      "3\n",
      "[1,     4] loss: 0.08599\n",
      "4\n",
      "[1,     5] loss: 0.08139\n",
      "5\n",
      "[1,     6] loss: 0.07603\n",
      "6\n",
      "[1,     7] loss: 0.07139\n",
      "7\n",
      "[1,     8] loss: 0.06652\n",
      "8\n",
      "[1,     9] loss: 0.06217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     12\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m accuracy_list \u001b[39m=\u001b[39m train(config, model, shakespeare_dataloader, optimizer, criterion)\n",
      "Cell \u001b[0;32mIn [22], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, model, train_dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     23\u001b[0m     rearrange(outputs, \u001b[39m'\u001b[39m\u001b[39mbatch seq vocab -> batch vocab seq\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     24\u001b[0m     labels\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=shakespeare_dataset.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_heads=4,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "model = DecoderTransformer(config)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "accuracy_list = train(config, model, shakespeare_dataloader, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
